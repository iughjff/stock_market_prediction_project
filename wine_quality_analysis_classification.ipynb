{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CFEbimVT1Jj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "shk5xym1YAjC",
        "outputId": "01a67910-4770-44d1-cc83-0d0f2b3c5117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a72d92c3-f779-45dc-9fff-891a48fd371d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a72d92c3-f779-45dc-9fff-891a48fd371d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving winequality.csv to winequality.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"winequality.csv\")"
      ],
      "metadata": {
        "id": "cmhqHFr6Y3T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "55By4gbxZHE6",
        "outputId": "6d925fb2-044e-425d-b44f-2bfa2f3803f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid residual sugar  chlorides  \\\n",
              "0            7.4              0.70         0.00            1.9      0.076   \n",
              "1            7.8              0.88         0.00            2.6      0.098   \n",
              "2            7.8              0.76         0.04            2.3      0.092   \n",
              "3           11.2              0.28         0.56            1.9      0.075   \n",
              "4           11.2              0.23         0.55            1.7      0.070   \n",
              "\n",
              "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
              "1                 25.0                  67.0   0.9968   3.2       0.68   \n",
              "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
              "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
              "4                 15.0                  60.0   0.9800     ?       0.50   \n",
              "\n",
              "   alcohol  quality  \n",
              "0      9.4        5  \n",
              "1      9.8        5  \n",
              "2      9.8        5  \n",
              "3      9.8        6  \n",
              "4      9.0        6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69ce5a09-c258-4ead-8757-b5163c8b7aa3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.2</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.55</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.070</td>\n",
              "      <td>15.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9800</td>\n",
              "      <td>?</td>\n",
              "      <td>0.50</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69ce5a09-c258-4ead-8757-b5163c8b7aa3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69ce5a09-c258-4ead-8757-b5163c8b7aa3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69ce5a09-c258-4ead-8757-b5163c8b7aa3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9768de69-63d2-4b4e-923c-b7f52c161c52\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9768de69-63d2-4b4e-923c-b7f52c161c52')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9768de69-63d2-4b4e-923c-b7f52c161c52 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "I5GdRiFEUi6L",
        "outputId": "b38c9806-cb3e-42e5-91ec-ba5f7b41bab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid residual sugar  chlorides  \\\n",
              "1598            6.2             0.600         0.08              2      0.090   \n",
              "1599            5.9             0.550         0.10            2.2      0.062   \n",
              "1600            6.3             0.510         0.13            2.3      0.076   \n",
              "1601            5.9             0.645         0.12              2      0.075   \n",
              "1602            6.0             0.310         0.47            3.6      0.067   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "1598                 32.0                  44.0  0.99490  3.45       0.58   \n",
              "1599                 39.0                  51.0  0.99512  3.52       0.76   \n",
              "1600                 29.0                  40.0  0.99574  3.42       0.75   \n",
              "1601                 32.0                  44.0  0.99547  3.57       0.71   \n",
              "1602                 18.0                  42.0  0.99549  3.39       0.66   \n",
              "\n",
              "      alcohol  quality  \n",
              "1598     10.5        5  \n",
              "1599     11.2        6  \n",
              "1600     11.0        6  \n",
              "1601     10.2        5  \n",
              "1602     11.0        6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df3d0013-a3e8-4a1e-9501-e26f1258e8f5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1601</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.47</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.067</td>\n",
              "      <td>18.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.99549</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df3d0013-a3e8-4a1e-9501-e26f1258e8f5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df3d0013-a3e8-4a1e-9501-e26f1258e8f5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df3d0013-a3e8-4a1e-9501-e26f1258e8f5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-30ee6b9c-1144-4201-9d7d-cfa9a487e64d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-30ee6b9c-1144-4201-9d7d-cfa9a487e64d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-30ee6b9c-1144-4201-9d7d-cfa9a487e64d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cek7AORjUkrt",
        "outputId": "7383fc66-deb7-4010-bf08-b811cd223caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1603, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnnwMrLgUpby",
        "outputId": "2a86bbbf-5d81-4572-9caa-4374d4073ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.describe of       fixed acidity  volatile acidity  citric acid residual sugar  chlorides  \\\n",
              "0               7.4             0.700         0.00            1.9      0.076   \n",
              "1               7.8             0.880         0.00            2.6      0.098   \n",
              "2               7.8             0.760         0.04            2.3      0.092   \n",
              "3              11.2             0.280         0.56            1.9      0.075   \n",
              "4              11.2             0.230         0.55            1.7      0.070   \n",
              "...             ...               ...          ...            ...        ...   \n",
              "1598            6.2             0.600         0.08              2      0.090   \n",
              "1599            5.9             0.550         0.10            2.2      0.062   \n",
              "1600            6.3             0.510         0.13            2.3      0.076   \n",
              "1601            5.9             0.645         0.12              2      0.075   \n",
              "1602            6.0             0.310         0.47            3.6      0.067   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
              "1                    25.0                  67.0  0.99680   3.2       0.68   \n",
              "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
              "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
              "4                    15.0                  60.0  0.98000     ?       0.50   \n",
              "...                   ...                   ...      ...   ...        ...   \n",
              "1598                 32.0                  44.0  0.99490  3.45       0.58   \n",
              "1599                 39.0                  51.0  0.99512  3.52       0.76   \n",
              "1600                 29.0                  40.0  0.99574  3.42       0.75   \n",
              "1601                 32.0                  44.0  0.99547  3.57       0.71   \n",
              "1602                 18.0                  42.0  0.99549  3.39       0.66   \n",
              "\n",
              "      alcohol  quality  \n",
              "0         9.4        5  \n",
              "1         9.8        5  \n",
              "2         9.8        5  \n",
              "3         9.8        6  \n",
              "4         9.0        6  \n",
              "...       ...      ...  \n",
              "1598     10.5        5  \n",
              "1599     11.2        6  \n",
              "1600     11.0        6  \n",
              "1601     10.2        5  \n",
              "1602     11.0        6  \n",
              "\n",
              "[1603 rows x 12 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgYSelbCUvCn",
        "outputId": "9f3dd46c-4dc8-44f7-900b-86722d2b42a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1603 entries, 0 to 1602\n",
            "Data columns (total 12 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   fixed acidity         1603 non-null   float64\n",
            " 1   volatile acidity      1603 non-null   float64\n",
            " 2   citric acid           1603 non-null   float64\n",
            " 3   residual sugar        1603 non-null   object \n",
            " 4   chlorides             1602 non-null   float64\n",
            " 5   free sulfur dioxide   1603 non-null   float64\n",
            " 6   total sulfur dioxide  1603 non-null   float64\n",
            " 7   density               1603 non-null   float64\n",
            " 8   pH                    1603 non-null   object \n",
            " 9   sulphates             1603 non-null   float64\n",
            " 10  alcohol               1603 non-null   float64\n",
            " 11  quality               1603 non-null   int64  \n",
            "dtypes: float64(9), int64(1), object(2)\n",
            "memory usage: 150.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['residual sugar'] = pd.to_numeric(data['residual sugar'], errors='coerce')\n",
        "data['pH'] = pd.to_numeric(data['pH'], errors='coerce')"
      ],
      "metadata": {
        "id": "gSSrdlnUU1Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtxHejUVU7PA",
        "outputId": "b1f41b5e-38c8-46e1-94fd-3a03e82dcc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fixed acidity           float64\n",
              "volatile acidity        float64\n",
              "citric acid             float64\n",
              "residual sugar          float64\n",
              "chlorides               float64\n",
              "free sulfur dioxide     float64\n",
              "total sulfur dioxide    float64\n",
              "density                 float64\n",
              "pH                      float64\n",
              "sulphates               float64\n",
              "alcohol                 float64\n",
              "quality                   int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIDLPRRzVBE6",
        "outputId": "81ed0fb1-8936-459b-8ee8-849277aaa1f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fixed acidity           0\n",
            "volatile acidity        0\n",
            "citric acid             0\n",
            "residual sugar          1\n",
            "chlorides               1\n",
            "free sulfur dioxide     0\n",
            "total sulfur dioxide    0\n",
            "density                 0\n",
            "pH                      1\n",
            "sulphates               0\n",
            "alcohol                 0\n",
            "quality                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "median_value = data['residual sugar'].median()\n",
        "data['residual sugar'].fillna(median_value, inplace=True)\n",
        "median_value = data['chlorides'].median()\n",
        "data['chlorides'].fillna(median_value, inplace=True)\n",
        "mean_value = data['pH'].mean()\n",
        "data['pH'].fillna(mean_value, inplace=True)\n"
      ],
      "metadata": {
        "id": "WQK5dCTlVFgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FirbNOd9VNt8",
        "outputId": "c2035a7d-6bc5-4636-effe-12d4ad87cfdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fixed acidity           0\n",
            "volatile acidity        0\n",
            "citric acid             0\n",
            "residual sugar          0\n",
            "chlorides               0\n",
            "free sulfur dioxide     0\n",
            "total sulfur dioxide    0\n",
            "density                 0\n",
            "pH                      0\n",
            "sulphates               0\n",
            "alcohol                 0\n",
            "quality                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['quality'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeKaTDb0W3Ao",
        "outputId": "5fe0b844-7a51-454a-9726-c04472544aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    682\n",
              "6    639\n",
              "7    199\n",
              "4     55\n",
              "8     18\n",
              "3     10\n",
              "Name: quality, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_value = 5\n",
        "data['Target'] = (data['quality'] > threshold_value).astype(int)\n",
        "data = data.drop('quality', axis=1)"
      ],
      "metadata": {
        "id": "v4NrwGRhVTVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "jkKtKRRvWRGp",
        "outputId": "0d740de0-f90c-4fdf-9dc0-41d2ada7bebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "0            7.4              0.70         0.00             1.9      0.076   \n",
              "1            7.8              0.88         0.00             2.6      0.098   \n",
              "2            7.8              0.76         0.04             2.3      0.092   \n",
              "3           11.2              0.28         0.56             1.9      0.075   \n",
              "4           11.2              0.23         0.55             1.7      0.070   \n",
              "\n",
              "   free sulfur dioxide  total sulfur dioxide  density        pH  sulphates  \\\n",
              "0                 11.0                  34.0   0.9978  3.510000       0.56   \n",
              "1                 25.0                  67.0   0.9968  3.200000       0.68   \n",
              "2                 15.0                  54.0   0.9970  3.260000       0.65   \n",
              "3                 17.0                  60.0   0.9980  3.160000       0.58   \n",
              "4                 15.0                  60.0   0.9800  3.311142       0.50   \n",
              "\n",
              "   alcohol  Target  \n",
              "0      9.4       0  \n",
              "1      9.8       0  \n",
              "2      9.8       0  \n",
              "3      9.8       1  \n",
              "4      9.0       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7355306-797f-4ae8-bd7d-3a94f559f0a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.510000</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.200000</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.260000</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.160000</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.55</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.070</td>\n",
              "      <td>15.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9800</td>\n",
              "      <td>3.311142</td>\n",
              "      <td>0.50</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7355306-797f-4ae8-bd7d-3a94f559f0a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7355306-797f-4ae8-bd7d-3a94f559f0a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7355306-797f-4ae8-bd7d-3a94f559f0a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c91a16de-05cf-424c-af85-224d0150e02a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c91a16de-05cf-424c-af85-224d0150e02a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c91a16de-05cf-424c-af85-224d0150e02a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otHvESiZ1J8_",
        "outputId": "980764bb-1ca1-4dbc-9457-ad4a7a42e42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    856\n",
              "0    747\n",
              "Name: Target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "sns.countplot(x='Target', data=data, palette='viridis')\n",
        "plt.title('Distribution of Target')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "qMLOedwv0wIW",
        "outputId": "e775f8f8-d19c-4df8-c415-dfe52babc92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASSIFICATION**"
      ],
      "metadata": {
        "id": "ADPBI-47XnCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting data into train test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "X = data.drop(['Target'], axis=1)\n",
        "y = data['Target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7TMBfoJXwLz",
        "outputId": "4eb8f87f-2f48-47e7-f0ba-2e775e3e3aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1202, 11), (401, 11))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "5T6WAvGknAfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "## Initialise the logistc regression model with:\n",
        "##   C (regularization parameter) = 1000\n",
        "lr = LogisticRegression(random_state = 0 )\n",
        "lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jUvS-zdNX6dy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "4e5609b9-241f-4105-c701-fc9bd8626a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)*100\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43DkdV2Uhya8",
        "outputId": "03ea5152-3db5-42e1-a655-3ce7f611a7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 71.3216957605985\n",
            "Confusion Matrix:\n",
            "[[123  69]\n",
            " [ 46 163]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.64      0.68       192\n",
            "           1       0.70      0.78      0.74       209\n",
            "\n",
            "    accuracy                           0.71       401\n",
            "   macro avg       0.72      0.71      0.71       401\n",
            "weighted avg       0.71      0.71      0.71       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUPPORT VECTOR MACHINE"
      ],
      "metadata": {
        "id": "rLi-SoVvnG0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel = 'linear',random_state = 0)\n",
        "svm.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "S4RX-rxIjDXT",
        "outputId": "4245882b-1e1d-47f6-fb72-72d2b37de249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear', random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred= svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)*100\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlil1Ok5jJTa",
        "outputId": "0f2f4fce-5f92-4f16-9609-6ff847367624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 72.81795511221945\n",
            "Confusion Matrix:\n",
            "[[133  59]\n",
            " [ 50 159]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.69      0.71       192\n",
            "           1       0.73      0.76      0.74       209\n",
            "\n",
            "    accuracy                           0.73       401\n",
            "   macro avg       0.73      0.73      0.73       401\n",
            "weighted avg       0.73      0.73      0.73       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECISION TREE CLASSIFIER"
      ],
      "metadata": {
        "id": "pIX4iR2InL_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(criterion='entropy'\n",
        "                             , max_depth = 3\n",
        "                             , random_state = 0)\n",
        "tree.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "UgrIZiLljkc_",
        "outputId": "380844b5-250d-4b14-f96c-e13bdee39e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=3, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = tree.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)*100\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7FrdJ_njpVK",
        "outputId": "78dc7768-9f56-4d52-c163-812aaa99d32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 67.08229426433915\n",
            "Confusion Matrix:\n",
            "[[ 85 107]\n",
            " [ 25 184]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.44      0.56       192\n",
            "           1       0.63      0.88      0.74       209\n",
            "\n",
            "    accuracy                           0.67       401\n",
            "   macro avg       0.70      0.66      0.65       401\n",
            "weighted avg       0.70      0.67      0.65       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAIVE BAYES CLASSIFIER"
      ],
      "metadata": {
        "id": "hMN-r85wnRLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "hfkm7YweloQl",
        "outputId": "5f09cc62-d3e2-4bf6-fb2c-4964a2918235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)*100\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgYI64U2l9h2",
        "outputId": "1c298d4a-1a90-44b5-eb42-82267219524d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.84788029925187\n",
            "Confusion Matrix:\n",
            "[[ 88 104]\n",
            " [ 53 156]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.46      0.53       192\n",
            "           1       0.60      0.75      0.67       209\n",
            "\n",
            "    accuracy                           0.61       401\n",
            "   macro avg       0.61      0.60      0.60       401\n",
            "weighted avg       0.61      0.61      0.60       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN CLASSIFIER"
      ],
      "metadata": {
        "id": "1NYmPFK8nWGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "k_value = 3\n",
        "model = KNeighborsClassifier(n_neighbors=k_value)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ckshvzpnmT6f",
        "outputId": "1c3f3c22-bbbb-408f-be84-c0bae1b4f16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)*100\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7yrI-o1mc8C",
        "outputId": "bb613578-956b-42a8-ba61-5e7f8a18932d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 66.58354114713218\n",
            "Confusion Matrix:\n",
            "[[123  69]\n",
            " [ 65 144]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.64      0.65       192\n",
            "           1       0.68      0.69      0.68       209\n",
            "\n",
            "    accuracy                           0.67       401\n",
            "   macro avg       0.67      0.66      0.66       401\n",
            "weighted avg       0.67      0.67      0.67       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RANDOM FOREST CLASSIFIER"
      ],
      "metadata": {
        "id": "3BcIKi0WndP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "H-MsP3NOngvB",
        "outputId": "e9ae49ed-abde-4fac-ae91-e2ace21fece9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)*100\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWmlHn8insDJ",
        "outputId": "edde451c-881b-4139-c84d-a4e103e7e472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 79.80049875311721\n",
            "Confusion Matrix:\n",
            "[[141  51]\n",
            " [ 30 179]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.73      0.78       192\n",
            "           1       0.78      0.86      0.82       209\n",
            "\n",
            "    accuracy                           0.80       401\n",
            "   macro avg       0.80      0.80      0.80       401\n",
            "weighted avg       0.80      0.80      0.80       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets check the accuracy of our model\n",
        "import numpy as np\n",
        "prediction = model.predict((np.array([[ 7.4, #fixed acidity\n",
        "                                        0.7, #'volatile acidity',\n",
        "                                        0, #'citric acid',\n",
        "                                        1.9, #'residual sugar',\n",
        "                                        0.076, #'chlorides',\n",
        "                                        11, #'free sulfur dioxide',\n",
        "                                        34, #'total sulfur dioxide',\n",
        "                                        0.9978, #'density',\n",
        "                                        3.51, #'pH',\n",
        "                                        0.56, #'sulphates',\n",
        "                                        9.4]])))#for alcohol\n",
        "print(\"The suggested quality for given features is:\",prediction)"
      ],
      "metadata": {
        "id": "_ClF-oaauH-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "480bfd07-1e7c-44b4-f632-86355209d600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The suggested quality for given features is: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaboost Classifier"
      ],
      "metadata": {
        "id": "GVxqutov3ZEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
        "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)"
      ],
      "metadata": {
        "id": "f1zfEylf20o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaboost_classifier.fit(X_train, y_train)\n",
        "y_pred = adaboost_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "ntBuNpYU3ByP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GEzAtOr3I6r",
        "outputId": "46dceebc-6c18-45e3-db01-663c00430b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7356608478802993\n",
            "Confusion Matrix:\n",
            "[[125  67]\n",
            " [ 39 170]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.65      0.70       192\n",
            "           1       0.72      0.81      0.76       209\n",
            "\n",
            "    accuracy                           0.74       401\n",
            "   macro avg       0.74      0.73      0.73       401\n",
            "weighted avg       0.74      0.74      0.73       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Class"
      ],
      "metadata": {
        "id": "4xJRNagl4F7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "xgb_classifier = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric=\"logloss\", random_state=42)\n"
      ],
      "metadata": {
        "id": "6grj4IkS3vvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_classifier.fit(X_train, y_train)\n",
        "y_pred = xgb_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "p7bfOIEo3xd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2v1GFtw3_dz",
        "outputId": "6fe2d2f7-6253-4bab-e356-95947394e430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7805486284289277\n",
            "Confusion Matrix:\n",
            "[[139  53]\n",
            " [ 35 174]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.72      0.76       192\n",
            "           1       0.77      0.83      0.80       209\n",
            "\n",
            "    accuracy                           0.78       401\n",
            "   macro avg       0.78      0.78      0.78       401\n",
            "weighted avg       0.78      0.78      0.78       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting"
      ],
      "metadata": {
        "id": "3ylR5LiqsooS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "V0gSf1qwsuHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "y_pred = gb_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "FoObuIILt3Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
        "print(f'Classification Report:\\n{class_report}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaN6Gs6rjRhV",
        "outputId": "05d591d7-142a-4ef6-8651-3f4e4fe2238e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7506234413965087\n",
            "Confusion Matrix:\n",
            "[[130  62]\n",
            " [ 38 171]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.68      0.72       192\n",
            "           1       0.73      0.82      0.77       209\n",
            "\n",
            "    accuracy                           0.75       401\n",
            "   macro avg       0.75      0.75      0.75       401\n",
            "weighted avg       0.75      0.75      0.75       401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neaural Network"
      ],
      "metadata": {
        "id": "xoguPoV_VFON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "-posi73mhWJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n"
      ],
      "metadata": {
        "id": "yGbSzkQwha1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "mr9-gEwaheeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adagrad(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "vn02AFJUhkBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U13mlxKhqpN",
        "outputId": "787911a8-9cce-41b1-91ea-08441191549e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "26/26 [==============================] - 0s 11ms/step - loss: 0.3215 - accuracy: 0.8582 - val_loss: 7.8984 - val_accuracy: 0.7703\n",
            "Epoch 2/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3216 - accuracy: 0.8582 - val_loss: 7.9040 - val_accuracy: 0.7703\n",
            "Epoch 3/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3213 - accuracy: 0.8594 - val_loss: 7.9120 - val_accuracy: 0.7703\n",
            "Epoch 4/1000\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.3221 - accuracy: 0.8522 - val_loss: 7.7751 - val_accuracy: 0.7703\n",
            "Epoch 5/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3218 - accuracy: 0.8546 - val_loss: 7.8390 - val_accuracy: 0.7656\n",
            "Epoch 6/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3217 - accuracy: 0.8546 - val_loss: 7.8547 - val_accuracy: 0.7703\n",
            "Epoch 7/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3214 - accuracy: 0.8570 - val_loss: 7.8407 - val_accuracy: 0.7656\n",
            "Epoch 8/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3215 - accuracy: 0.8582 - val_loss: 7.8471 - val_accuracy: 0.7703\n",
            "Epoch 9/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3215 - accuracy: 0.8582 - val_loss: 7.8631 - val_accuracy: 0.7703\n",
            "Epoch 10/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3215 - accuracy: 0.8534 - val_loss: 7.8742 - val_accuracy: 0.7656\n",
            "Epoch 11/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3214 - accuracy: 0.8594 - val_loss: 7.8849 - val_accuracy: 0.7703\n",
            "Epoch 12/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3215 - accuracy: 0.8594 - val_loss: 7.8883 - val_accuracy: 0.7703\n",
            "Epoch 13/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3214 - accuracy: 0.8534 - val_loss: 7.8802 - val_accuracy: 0.7703\n",
            "Epoch 14/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3215 - accuracy: 0.8558 - val_loss: 7.8827 - val_accuracy: 0.7703\n",
            "Epoch 15/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3217 - accuracy: 0.8582 - val_loss: 7.8915 - val_accuracy: 0.7703\n",
            "Epoch 16/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3215 - accuracy: 0.8570 - val_loss: 7.9004 - val_accuracy: 0.7703\n",
            "Epoch 17/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3216 - accuracy: 0.8582 - val_loss: 7.9066 - val_accuracy: 0.7656\n",
            "Epoch 18/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3217 - accuracy: 0.8534 - val_loss: 7.9047 - val_accuracy: 0.7656\n",
            "Epoch 19/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3216 - accuracy: 0.8570 - val_loss: 7.9050 - val_accuracy: 0.7656\n",
            "Epoch 20/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3215 - accuracy: 0.8570 - val_loss: 7.9045 - val_accuracy: 0.7656\n",
            "Epoch 21/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3215 - accuracy: 0.8582 - val_loss: 7.9143 - val_accuracy: 0.7703\n",
            "Epoch 22/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3214 - accuracy: 0.8570 - val_loss: 7.9300 - val_accuracy: 0.7656\n",
            "Epoch 23/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3213 - accuracy: 0.8558 - val_loss: 7.9295 - val_accuracy: 0.7656\n",
            "Epoch 24/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3213 - accuracy: 0.8570 - val_loss: 7.9425 - val_accuracy: 0.7656\n",
            "Epoch 25/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.8558 - val_loss: 7.9383 - val_accuracy: 0.7703\n",
            "Epoch 26/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.8582 - val_loss: 7.9383 - val_accuracy: 0.7703\n",
            "Epoch 27/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8558 - val_loss: 7.9448 - val_accuracy: 0.7656\n",
            "Epoch 28/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8570 - val_loss: 7.9367 - val_accuracy: 0.7703\n",
            "Epoch 29/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8582 - val_loss: 7.9383 - val_accuracy: 0.7703\n",
            "Epoch 30/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8570 - val_loss: 7.9148 - val_accuracy: 0.7656\n",
            "Epoch 31/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.8570 - val_loss: 7.9359 - val_accuracy: 0.7656\n",
            "Epoch 32/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8570 - val_loss: 7.9450 - val_accuracy: 0.7656\n",
            "Epoch 33/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8558 - val_loss: 7.9395 - val_accuracy: 0.7656\n",
            "Epoch 34/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.8594 - val_loss: 7.9575 - val_accuracy: 0.7656\n",
            "Epoch 35/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8594 - val_loss: 7.9620 - val_accuracy: 0.7703\n",
            "Epoch 36/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3214 - accuracy: 0.8558 - val_loss: 7.9562 - val_accuracy: 0.7703\n",
            "Epoch 37/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.8558 - val_loss: 7.9626 - val_accuracy: 0.7703\n",
            "Epoch 38/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.8594 - val_loss: 7.9701 - val_accuracy: 0.7703\n",
            "Epoch 39/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8570 - val_loss: 7.9722 - val_accuracy: 0.7703\n",
            "Epoch 40/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8582 - val_loss: 7.9656 - val_accuracy: 0.7703\n",
            "Epoch 41/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8570 - val_loss: 7.9746 - val_accuracy: 0.7703\n",
            "Epoch 42/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.8582 - val_loss: 7.9852 - val_accuracy: 0.7703\n",
            "Epoch 43/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8546 - val_loss: 7.9856 - val_accuracy: 0.7656\n",
            "Epoch 44/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8546 - val_loss: 7.9952 - val_accuracy: 0.7703\n",
            "Epoch 45/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8570 - val_loss: 7.9875 - val_accuracy: 0.7703\n",
            "Epoch 46/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8570 - val_loss: 7.9997 - val_accuracy: 0.7703\n",
            "Epoch 47/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8546 - val_loss: 7.9974 - val_accuracy: 0.7703\n",
            "Epoch 48/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8546 - val_loss: 8.0038 - val_accuracy: 0.7703\n",
            "Epoch 49/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.8558 - val_loss: 8.0654 - val_accuracy: 0.7656\n",
            "Epoch 50/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8546 - val_loss: 8.0517 - val_accuracy: 0.7703\n",
            "Epoch 51/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3216 - accuracy: 0.8570 - val_loss: 8.0396 - val_accuracy: 0.7703\n",
            "Epoch 52/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8546 - val_loss: 8.0388 - val_accuracy: 0.7656\n",
            "Epoch 53/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.8582 - val_loss: 8.0279 - val_accuracy: 0.7656\n",
            "Epoch 54/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8558 - val_loss: 8.0434 - val_accuracy: 0.7656\n",
            "Epoch 55/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3214 - accuracy: 0.8558 - val_loss: 8.0409 - val_accuracy: 0.7703\n",
            "Epoch 56/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8546 - val_loss: 8.0181 - val_accuracy: 0.7703\n",
            "Epoch 57/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8534 - val_loss: 8.0475 - val_accuracy: 0.7751\n",
            "Epoch 58/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8546 - val_loss: 8.0316 - val_accuracy: 0.7751\n",
            "Epoch 59/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3212 - accuracy: 0.8546 - val_loss: 8.0374 - val_accuracy: 0.7751\n",
            "Epoch 60/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3212 - accuracy: 0.8534 - val_loss: 8.0771 - val_accuracy: 0.7751\n",
            "Epoch 61/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3214 - accuracy: 0.8570 - val_loss: 8.0664 - val_accuracy: 0.7751\n",
            "Epoch 62/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3212 - accuracy: 0.8546 - val_loss: 8.0609 - val_accuracy: 0.7751\n",
            "Epoch 63/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.8534 - val_loss: 8.0706 - val_accuracy: 0.7703\n",
            "Epoch 64/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3211 - accuracy: 0.8534 - val_loss: 8.0693 - val_accuracy: 0.7751\n",
            "Epoch 65/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.8534 - val_loss: 8.0844 - val_accuracy: 0.7751\n",
            "Epoch 66/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.8534 - val_loss: 8.0793 - val_accuracy: 0.7751\n",
            "Epoch 67/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3209 - accuracy: 0.8546 - val_loss: 8.0926 - val_accuracy: 0.7751\n",
            "Epoch 68/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.8534 - val_loss: 8.0964 - val_accuracy: 0.7751\n",
            "Epoch 69/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3213 - accuracy: 0.8546 - val_loss: 7.9730 - val_accuracy: 0.7751\n",
            "Epoch 70/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8570 - val_loss: 8.0128 - val_accuracy: 0.7751\n",
            "Epoch 71/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3212 - accuracy: 0.8522 - val_loss: 8.0756 - val_accuracy: 0.7751\n",
            "Epoch 72/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3207 - accuracy: 0.8546 - val_loss: 8.0954 - val_accuracy: 0.7751\n",
            "Epoch 73/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8534 - val_loss: 8.1004 - val_accuracy: 0.7751\n",
            "Epoch 74/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8546 - val_loss: 8.1203 - val_accuracy: 0.7703\n",
            "Epoch 75/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3210 - accuracy: 0.8522 - val_loss: 8.1128 - val_accuracy: 0.7703\n",
            "Epoch 76/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3209 - accuracy: 0.8546 - val_loss: 8.1508 - val_accuracy: 0.7703\n",
            "Epoch 77/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3212 - accuracy: 0.8534 - val_loss: 8.1323 - val_accuracy: 0.7751\n",
            "Epoch 78/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3208 - accuracy: 0.8534 - val_loss: 8.1200 - val_accuracy: 0.7703\n",
            "Epoch 79/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3210 - accuracy: 0.8534 - val_loss: 8.1313 - val_accuracy: 0.7751\n",
            "Epoch 80/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8522 - val_loss: 8.1152 - val_accuracy: 0.7751\n",
            "Epoch 81/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8522 - val_loss: 8.1305 - val_accuracy: 0.7751\n",
            "Epoch 82/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3208 - accuracy: 0.8546 - val_loss: 8.1087 - val_accuracy: 0.7751\n",
            "Epoch 83/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3206 - accuracy: 0.8534 - val_loss: 8.1337 - val_accuracy: 0.7751\n",
            "Epoch 84/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8534 - val_loss: 8.0082 - val_accuracy: 0.7703\n",
            "Epoch 85/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3212 - accuracy: 0.8534 - val_loss: 8.0847 - val_accuracy: 0.7751\n",
            "Epoch 86/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3206 - accuracy: 0.8534 - val_loss: 8.1198 - val_accuracy: 0.7703\n",
            "Epoch 87/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3207 - accuracy: 0.8534 - val_loss: 8.1283 - val_accuracy: 0.7703\n",
            "Epoch 88/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3206 - accuracy: 0.8510 - val_loss: 8.1324 - val_accuracy: 0.7703\n",
            "Epoch 89/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3204 - accuracy: 0.8522 - val_loss: 8.1352 - val_accuracy: 0.7703\n",
            "Epoch 90/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3206 - accuracy: 0.8546 - val_loss: 8.1405 - val_accuracy: 0.7703\n",
            "Epoch 91/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3209 - accuracy: 0.8510 - val_loss: 8.1609 - val_accuracy: 0.7703\n",
            "Epoch 92/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3205 - accuracy: 0.8522 - val_loss: 8.1593 - val_accuracy: 0.7703\n",
            "Epoch 93/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3205 - accuracy: 0.8534 - val_loss: 8.1615 - val_accuracy: 0.7703\n",
            "Epoch 94/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3207 - accuracy: 0.8522 - val_loss: 8.1679 - val_accuracy: 0.7703\n",
            "Epoch 95/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8522 - val_loss: 8.1794 - val_accuracy: 0.7703\n",
            "Epoch 96/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8510 - val_loss: 8.1956 - val_accuracy: 0.7656\n",
            "Epoch 97/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8546 - val_loss: 8.1835 - val_accuracy: 0.7703\n",
            "Epoch 98/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8546 - val_loss: 8.0637 - val_accuracy: 0.7751\n",
            "Epoch 99/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3208 - accuracy: 0.8522 - val_loss: 8.0964 - val_accuracy: 0.7751\n",
            "Epoch 100/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3207 - accuracy: 0.8534 - val_loss: 8.1638 - val_accuracy: 0.7703\n",
            "Epoch 101/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3206 - accuracy: 0.8522 - val_loss: 8.1664 - val_accuracy: 0.7751\n",
            "Epoch 102/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8558 - val_loss: 8.1641 - val_accuracy: 0.7656\n",
            "Epoch 103/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8522 - val_loss: 8.1679 - val_accuracy: 0.7656\n",
            "Epoch 104/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8522 - val_loss: 8.1806 - val_accuracy: 0.7656\n",
            "Epoch 105/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8534 - val_loss: 8.1784 - val_accuracy: 0.7751\n",
            "Epoch 106/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8546 - val_loss: 8.2121 - val_accuracy: 0.7703\n",
            "Epoch 107/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8522 - val_loss: 8.2347 - val_accuracy: 0.7703\n",
            "Epoch 108/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8534 - val_loss: 8.2234 - val_accuracy: 0.7656\n",
            "Epoch 109/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8522 - val_loss: 8.2316 - val_accuracy: 0.7656\n",
            "Epoch 110/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8558 - val_loss: 8.2360 - val_accuracy: 0.7751\n",
            "Epoch 111/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8534 - val_loss: 8.2441 - val_accuracy: 0.7703\n",
            "Epoch 112/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8522 - val_loss: 8.2496 - val_accuracy: 0.7751\n",
            "Epoch 113/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8534 - val_loss: 8.2624 - val_accuracy: 0.7751\n",
            "Epoch 114/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8522 - val_loss: 8.2531 - val_accuracy: 0.7751\n",
            "Epoch 115/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8546 - val_loss: 8.2739 - val_accuracy: 0.7656\n",
            "Epoch 116/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8546 - val_loss: 8.2883 - val_accuracy: 0.7656\n",
            "Epoch 117/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8534 - val_loss: 8.2855 - val_accuracy: 0.7751\n",
            "Epoch 118/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8522 - val_loss: 8.2918 - val_accuracy: 0.7703\n",
            "Epoch 119/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3204 - accuracy: 0.8570 - val_loss: 8.3176 - val_accuracy: 0.7703\n",
            "Epoch 120/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8522 - val_loss: 8.3073 - val_accuracy: 0.7703\n",
            "Epoch 121/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8522 - val_loss: 8.2820 - val_accuracy: 0.7751\n",
            "Epoch 122/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8546 - val_loss: 8.2915 - val_accuracy: 0.7751\n",
            "Epoch 123/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8522 - val_loss: 8.3008 - val_accuracy: 0.7703\n",
            "Epoch 124/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3198 - accuracy: 0.8558 - val_loss: 8.2961 - val_accuracy: 0.7751\n",
            "Epoch 125/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3206 - accuracy: 0.8522 - val_loss: 8.1925 - val_accuracy: 0.7751\n",
            "Epoch 126/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8534 - val_loss: 8.2712 - val_accuracy: 0.7751\n",
            "Epoch 127/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.8534 - val_loss: 8.2943 - val_accuracy: 0.7703\n",
            "Epoch 128/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8534 - val_loss: 8.3220 - val_accuracy: 0.7703\n",
            "Epoch 129/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8510 - val_loss: 8.3100 - val_accuracy: 0.7703\n",
            "Epoch 130/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8534 - val_loss: 8.3314 - val_accuracy: 0.7703\n",
            "Epoch 131/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8546 - val_loss: 8.3386 - val_accuracy: 0.7656\n",
            "Epoch 132/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8534 - val_loss: 8.3582 - val_accuracy: 0.7703\n",
            "Epoch 133/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.8522 - val_loss: 8.3597 - val_accuracy: 0.7751\n",
            "Epoch 134/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8546 - val_loss: 8.3539 - val_accuracy: 0.7751\n",
            "Epoch 135/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8510 - val_loss: 8.3625 - val_accuracy: 0.7703\n",
            "Epoch 136/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8522 - val_loss: 8.3719 - val_accuracy: 0.7656\n",
            "Epoch 137/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8546 - val_loss: 8.3807 - val_accuracy: 0.7751\n",
            "Epoch 138/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.8534 - val_loss: 8.4056 - val_accuracy: 0.7703\n",
            "Epoch 139/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8522 - val_loss: 8.3956 - val_accuracy: 0.7703\n",
            "Epoch 140/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.8534 - val_loss: 8.3935 - val_accuracy: 0.7703\n",
            "Epoch 141/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8522 - val_loss: 8.4087 - val_accuracy: 0.7656\n",
            "Epoch 142/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8546 - val_loss: 8.4179 - val_accuracy: 0.7703\n",
            "Epoch 143/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8522 - val_loss: 8.2740 - val_accuracy: 0.7751\n",
            "Epoch 144/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8546 - val_loss: 8.3408 - val_accuracy: 0.7751\n",
            "Epoch 145/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8534 - val_loss: 8.3672 - val_accuracy: 0.7703\n",
            "Epoch 146/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.8522 - val_loss: 8.4091 - val_accuracy: 0.7703\n",
            "Epoch 147/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8534 - val_loss: 8.3970 - val_accuracy: 0.7751\n",
            "Epoch 148/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3195 - accuracy: 0.8558 - val_loss: 8.4075 - val_accuracy: 0.7703\n",
            "Epoch 149/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8534 - val_loss: 8.4076 - val_accuracy: 0.7703\n",
            "Epoch 150/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8558 - val_loss: 8.4001 - val_accuracy: 0.7751\n",
            "Epoch 151/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8546 - val_loss: 8.3853 - val_accuracy: 0.7751\n",
            "Epoch 152/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8546 - val_loss: 8.3947 - val_accuracy: 0.7751\n",
            "Epoch 153/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8534 - val_loss: 8.3944 - val_accuracy: 0.7751\n",
            "Epoch 154/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8510 - val_loss: 8.4050 - val_accuracy: 0.7751\n",
            "Epoch 155/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.8522 - val_loss: 8.4222 - val_accuracy: 0.7703\n",
            "Epoch 156/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8546 - val_loss: 8.4390 - val_accuracy: 0.7703\n",
            "Epoch 157/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8534 - val_loss: 8.4422 - val_accuracy: 0.7751\n",
            "Epoch 158/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8570 - val_loss: 8.4428 - val_accuracy: 0.7751\n",
            "Epoch 159/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8558 - val_loss: 8.4489 - val_accuracy: 0.7703\n",
            "Epoch 160/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.8534 - val_loss: 8.5729 - val_accuracy: 0.7703\n",
            "Epoch 161/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3223 - accuracy: 0.8546 - val_loss: 8.3663 - val_accuracy: 0.7703\n",
            "Epoch 162/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8534 - val_loss: 8.3969 - val_accuracy: 0.7751\n",
            "Epoch 163/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8534 - val_loss: 8.4165 - val_accuracy: 0.7751\n",
            "Epoch 164/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.8534 - val_loss: 8.4212 - val_accuracy: 0.7751\n",
            "Epoch 165/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3192 - accuracy: 0.8558 - val_loss: 8.4290 - val_accuracy: 0.7751\n",
            "Epoch 166/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8534 - val_loss: 8.4609 - val_accuracy: 0.7703\n",
            "Epoch 167/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3199 - accuracy: 0.8546 - val_loss: 8.4492 - val_accuracy: 0.7751\n",
            "Epoch 168/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8558 - val_loss: 8.4500 - val_accuracy: 0.7751\n",
            "Epoch 169/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8558 - val_loss: 8.4725 - val_accuracy: 0.7751\n",
            "Epoch 170/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8546 - val_loss: 8.4730 - val_accuracy: 0.7751\n",
            "Epoch 171/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8546 - val_loss: 8.4916 - val_accuracy: 0.7751\n",
            "Epoch 172/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.8534 - val_loss: 8.4377 - val_accuracy: 0.7751\n",
            "Epoch 173/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3202 - accuracy: 0.8534 - val_loss: 8.2423 - val_accuracy: 0.7703\n",
            "Epoch 174/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3211 - accuracy: 0.8558 - val_loss: 8.2944 - val_accuracy: 0.7703\n",
            "Epoch 175/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8546 - val_loss: 8.3401 - val_accuracy: 0.7656\n",
            "Epoch 176/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.8546 - val_loss: 8.3566 - val_accuracy: 0.7703\n",
            "Epoch 177/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8522 - val_loss: 8.3571 - val_accuracy: 0.7751\n",
            "Epoch 178/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3200 - accuracy: 0.8546 - val_loss: 8.3926 - val_accuracy: 0.7751\n",
            "Epoch 179/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3199 - accuracy: 0.8522 - val_loss: 8.4014 - val_accuracy: 0.7751\n",
            "Epoch 180/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.8558 - val_loss: 8.4086 - val_accuracy: 0.7751\n",
            "Epoch 181/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3198 - accuracy: 0.8546 - val_loss: 8.4290 - val_accuracy: 0.7751\n",
            "Epoch 182/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3197 - accuracy: 0.8534 - val_loss: 8.4898 - val_accuracy: 0.7656\n",
            "Epoch 183/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3192 - accuracy: 0.8522 - val_loss: 8.5040 - val_accuracy: 0.7751\n",
            "Epoch 184/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3191 - accuracy: 0.8546 - val_loss: 8.5035 - val_accuracy: 0.7703\n",
            "Epoch 185/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8546 - val_loss: 8.5333 - val_accuracy: 0.7656\n",
            "Epoch 186/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3191 - accuracy: 0.8546 - val_loss: 8.5143 - val_accuracy: 0.7656\n",
            "Epoch 187/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.8546 - val_loss: 8.5169 - val_accuracy: 0.7703\n",
            "Epoch 188/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3191 - accuracy: 0.8546 - val_loss: 8.5145 - val_accuracy: 0.7751\n",
            "Epoch 189/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3190 - accuracy: 0.8558 - val_loss: 8.5277 - val_accuracy: 0.7751\n",
            "Epoch 190/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3188 - accuracy: 0.8558 - val_loss: 8.5327 - val_accuracy: 0.7751\n",
            "Epoch 191/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.8534 - val_loss: 8.5052 - val_accuracy: 0.7703\n",
            "Epoch 192/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3188 - accuracy: 0.8546 - val_loss: 8.5166 - val_accuracy: 0.7703\n",
            "Epoch 193/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8534 - val_loss: 8.5190 - val_accuracy: 0.7751\n",
            "Epoch 194/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8534 - val_loss: 8.5410 - val_accuracy: 0.7703\n",
            "Epoch 195/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3191 - accuracy: 0.8546 - val_loss: 8.5147 - val_accuracy: 0.7751\n",
            "Epoch 196/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8546 - val_loss: 8.5227 - val_accuracy: 0.7751\n",
            "Epoch 197/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8534 - val_loss: 8.5277 - val_accuracy: 0.7751\n",
            "Epoch 198/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8570 - val_loss: 8.5318 - val_accuracy: 0.7751\n",
            "Epoch 199/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3190 - accuracy: 0.8546 - val_loss: 8.5535 - val_accuracy: 0.7751\n",
            "Epoch 200/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3191 - accuracy: 0.8546 - val_loss: 8.5415 - val_accuracy: 0.7751\n",
            "Epoch 201/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.8546 - val_loss: 8.5536 - val_accuracy: 0.7751\n",
            "Epoch 202/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8546 - val_loss: 8.5558 - val_accuracy: 0.7751\n",
            "Epoch 203/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3186 - accuracy: 0.8546 - val_loss: 8.5537 - val_accuracy: 0.7751\n",
            "Epoch 204/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8558 - val_loss: 8.5603 - val_accuracy: 0.7751\n",
            "Epoch 205/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3186 - accuracy: 0.8570 - val_loss: 8.5531 - val_accuracy: 0.7703\n",
            "Epoch 206/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3188 - accuracy: 0.8546 - val_loss: 8.5585 - val_accuracy: 0.7703\n",
            "Epoch 207/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.8558 - val_loss: 8.5881 - val_accuracy: 0.7751\n",
            "Epoch 208/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3189 - accuracy: 0.8558 - val_loss: 8.5801 - val_accuracy: 0.7751\n",
            "Epoch 209/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3186 - accuracy: 0.8546 - val_loss: 8.4516 - val_accuracy: 0.7703\n",
            "Epoch 210/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.8546 - val_loss: 8.5232 - val_accuracy: 0.7751\n",
            "Epoch 211/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3188 - accuracy: 0.8558 - val_loss: 8.5460 - val_accuracy: 0.7751\n",
            "Epoch 212/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8570 - val_loss: 8.5690 - val_accuracy: 0.7703\n",
            "Epoch 213/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8546 - val_loss: 8.5673 - val_accuracy: 0.7751\n",
            "Epoch 214/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8558 - val_loss: 8.5613 - val_accuracy: 0.7751\n",
            "Epoch 215/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8546 - val_loss: 8.5765 - val_accuracy: 0.7703\n",
            "Epoch 216/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8558 - val_loss: 8.6208 - val_accuracy: 0.7703\n",
            "Epoch 217/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3189 - accuracy: 0.8546 - val_loss: 8.5930 - val_accuracy: 0.7751\n",
            "Epoch 218/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3189 - accuracy: 0.8546 - val_loss: 8.5945 - val_accuracy: 0.7751\n",
            "Epoch 219/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8558 - val_loss: 8.6018 - val_accuracy: 0.7751\n",
            "Epoch 220/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8522 - val_loss: 8.6063 - val_accuracy: 0.7751\n",
            "Epoch 221/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8534 - val_loss: 8.6065 - val_accuracy: 0.7751\n",
            "Epoch 222/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8558 - val_loss: 8.6252 - val_accuracy: 0.7703\n",
            "Epoch 223/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.8558 - val_loss: 8.6202 - val_accuracy: 0.7751\n",
            "Epoch 224/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8558 - val_loss: 8.6112 - val_accuracy: 0.7751\n",
            "Epoch 225/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8546 - val_loss: 8.6172 - val_accuracy: 0.7703\n",
            "Epoch 226/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8558 - val_loss: 8.6321 - val_accuracy: 0.7751\n",
            "Epoch 227/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8558 - val_loss: 8.6259 - val_accuracy: 0.7751\n",
            "Epoch 228/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8558 - val_loss: 8.6242 - val_accuracy: 0.7751\n",
            "Epoch 229/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8546 - val_loss: 8.6361 - val_accuracy: 0.7751\n",
            "Epoch 230/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8558 - val_loss: 8.6421 - val_accuracy: 0.7656\n",
            "Epoch 231/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8558 - val_loss: 8.6140 - val_accuracy: 0.7751\n",
            "Epoch 232/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8558 - val_loss: 8.6353 - val_accuracy: 0.7751\n",
            "Epoch 233/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8558 - val_loss: 8.6547 - val_accuracy: 0.7751\n",
            "Epoch 234/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8570 - val_loss: 8.6373 - val_accuracy: 0.7751\n",
            "Epoch 235/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8582 - val_loss: 8.6489 - val_accuracy: 0.7751\n",
            "Epoch 236/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8582 - val_loss: 8.6660 - val_accuracy: 0.7751\n",
            "Epoch 237/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8534 - val_loss: 8.6579 - val_accuracy: 0.7751\n",
            "Epoch 238/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8558 - val_loss: 8.6602 - val_accuracy: 0.7751\n",
            "Epoch 239/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8558 - val_loss: 8.6628 - val_accuracy: 0.7751\n",
            "Epoch 240/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8582 - val_loss: 8.6365 - val_accuracy: 0.7751\n",
            "Epoch 241/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8606 - val_loss: 8.6451 - val_accuracy: 0.7751\n",
            "Epoch 242/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8570 - val_loss: 8.6529 - val_accuracy: 0.7751\n",
            "Epoch 243/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8558 - val_loss: 8.6630 - val_accuracy: 0.7751\n",
            "Epoch 244/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3191 - accuracy: 0.8534 - val_loss: 8.5714 - val_accuracy: 0.7703\n",
            "Epoch 245/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8558 - val_loss: 8.6148 - val_accuracy: 0.7751\n",
            "Epoch 246/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8570 - val_loss: 8.6339 - val_accuracy: 0.7751\n",
            "Epoch 247/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8558 - val_loss: 8.6245 - val_accuracy: 0.7751\n",
            "Epoch 248/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8546 - val_loss: 8.6404 - val_accuracy: 0.7751\n",
            "Epoch 249/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8546 - val_loss: 8.6503 - val_accuracy: 0.7751\n",
            "Epoch 250/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8546 - val_loss: 8.6744 - val_accuracy: 0.7703\n",
            "Epoch 251/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8570 - val_loss: 8.6775 - val_accuracy: 0.7751\n",
            "Epoch 252/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8558 - val_loss: 8.6482 - val_accuracy: 0.7751\n",
            "Epoch 253/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8546 - val_loss: 8.6545 - val_accuracy: 0.7751\n",
            "Epoch 254/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8534 - val_loss: 8.6635 - val_accuracy: 0.7751\n",
            "Epoch 255/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8546 - val_loss: 8.6671 - val_accuracy: 0.7751\n",
            "Epoch 256/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3182 - accuracy: 0.8558 - val_loss: 8.6800 - val_accuracy: 0.7751\n",
            "Epoch 257/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8534 - val_loss: 8.7111 - val_accuracy: 0.7703\n",
            "Epoch 258/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8558 - val_loss: 8.7014 - val_accuracy: 0.7703\n",
            "Epoch 259/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8558 - val_loss: 8.7221 - val_accuracy: 0.7751\n",
            "Epoch 260/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8534 - val_loss: 8.7230 - val_accuracy: 0.7751\n",
            "Epoch 261/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3181 - accuracy: 0.8582 - val_loss: 8.7338 - val_accuracy: 0.7703\n",
            "Epoch 262/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3180 - accuracy: 0.8546 - val_loss: 8.7297 - val_accuracy: 0.7751\n",
            "Epoch 263/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8558 - val_loss: 8.6091 - val_accuracy: 0.7703\n",
            "Epoch 264/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8582 - val_loss: 8.6891 - val_accuracy: 0.7703\n",
            "Epoch 265/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8570 - val_loss: 8.6865 - val_accuracy: 0.7751\n",
            "Epoch 266/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3182 - accuracy: 0.8546 - val_loss: 8.7270 - val_accuracy: 0.7703\n",
            "Epoch 267/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3185 - accuracy: 0.8546 - val_loss: 8.7208 - val_accuracy: 0.7703\n",
            "Epoch 268/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8558 - val_loss: 8.7061 - val_accuracy: 0.7751\n",
            "Epoch 269/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8558 - val_loss: 8.7329 - val_accuracy: 0.7751\n",
            "Epoch 270/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8558 - val_loss: 8.7433 - val_accuracy: 0.7656\n",
            "Epoch 271/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8570 - val_loss: 8.7450 - val_accuracy: 0.7751\n",
            "Epoch 272/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.8546 - val_loss: 8.7382 - val_accuracy: 0.7751\n",
            "Epoch 273/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3180 - accuracy: 0.8546 - val_loss: 8.7524 - val_accuracy: 0.7656\n",
            "Epoch 274/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8546 - val_loss: 8.7501 - val_accuracy: 0.7703\n",
            "Epoch 275/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8582 - val_loss: 8.7466 - val_accuracy: 0.7751\n",
            "Epoch 276/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8558 - val_loss: 8.7950 - val_accuracy: 0.7656\n",
            "Epoch 277/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8558 - val_loss: 8.7602 - val_accuracy: 0.7703\n",
            "Epoch 278/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8534 - val_loss: 8.7672 - val_accuracy: 0.7751\n",
            "Epoch 279/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8534 - val_loss: 8.7632 - val_accuracy: 0.7751\n",
            "Epoch 280/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8534 - val_loss: 8.7653 - val_accuracy: 0.7703\n",
            "Epoch 281/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8570 - val_loss: 8.7412 - val_accuracy: 0.7751\n",
            "Epoch 282/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8570 - val_loss: 8.6471 - val_accuracy: 0.7703\n",
            "Epoch 283/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3187 - accuracy: 0.8546 - val_loss: 8.7077 - val_accuracy: 0.7703\n",
            "Epoch 284/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8570 - val_loss: 8.7210 - val_accuracy: 0.7656\n",
            "Epoch 285/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8534 - val_loss: 8.7293 - val_accuracy: 0.7656\n",
            "Epoch 286/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8558 - val_loss: 8.7501 - val_accuracy: 0.7751\n",
            "Epoch 287/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8558 - val_loss: 8.7647 - val_accuracy: 0.7751\n",
            "Epoch 288/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8558 - val_loss: 8.7985 - val_accuracy: 0.7703\n",
            "Epoch 289/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8558 - val_loss: 8.7853 - val_accuracy: 0.7751\n",
            "Epoch 290/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.8546 - val_loss: 8.7851 - val_accuracy: 0.7751\n",
            "Epoch 291/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.8546 - val_loss: 8.7813 - val_accuracy: 0.7751\n",
            "Epoch 292/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8534 - val_loss: 8.7810 - val_accuracy: 0.7751\n",
            "Epoch 293/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8558 - val_loss: 8.7827 - val_accuracy: 0.7751\n",
            "Epoch 294/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.8534 - val_loss: 8.7854 - val_accuracy: 0.7751\n",
            "Epoch 295/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.8570 - val_loss: 8.7912 - val_accuracy: 0.7751\n",
            "Epoch 296/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.8546 - val_loss: 8.8251 - val_accuracy: 0.7703\n",
            "Epoch 297/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3179 - accuracy: 0.8546 - val_loss: 8.8067 - val_accuracy: 0.7751\n",
            "Epoch 298/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3180 - accuracy: 0.8582 - val_loss: 8.8076 - val_accuracy: 0.7751\n",
            "Epoch 299/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3177 - accuracy: 0.8546 - val_loss: 8.7967 - val_accuracy: 0.7751\n",
            "Epoch 300/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3179 - accuracy: 0.8546 - val_loss: 8.7979 - val_accuracy: 0.7751\n",
            "Epoch 301/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3176 - accuracy: 0.8558 - val_loss: 8.8047 - val_accuracy: 0.7751\n",
            "Epoch 302/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8558 - val_loss: 8.8073 - val_accuracy: 0.7751\n",
            "Epoch 303/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3177 - accuracy: 0.8558 - val_loss: 8.8072 - val_accuracy: 0.7751\n",
            "Epoch 304/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3176 - accuracy: 0.8582 - val_loss: 8.7980 - val_accuracy: 0.7751\n",
            "Epoch 305/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3177 - accuracy: 0.8558 - val_loss: 8.8186 - val_accuracy: 0.7751\n",
            "Epoch 306/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3178 - accuracy: 0.8570 - val_loss: 8.8596 - val_accuracy: 0.7703\n",
            "Epoch 307/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3179 - accuracy: 0.8558 - val_loss: 8.8119 - val_accuracy: 0.7751\n",
            "Epoch 308/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3176 - accuracy: 0.8570 - val_loss: 8.8278 - val_accuracy: 0.7703\n",
            "Epoch 309/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3180 - accuracy: 0.8558 - val_loss: 8.8199 - val_accuracy: 0.7751\n",
            "Epoch 310/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.8558 - val_loss: 8.8210 - val_accuracy: 0.7751\n",
            "Epoch 311/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3176 - accuracy: 0.8558 - val_loss: 8.8245 - val_accuracy: 0.7751\n",
            "Epoch 312/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3176 - accuracy: 0.8570 - val_loss: 8.8364 - val_accuracy: 0.7751\n",
            "Epoch 313/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3177 - accuracy: 0.8570 - val_loss: 8.8318 - val_accuracy: 0.7751\n",
            "Epoch 314/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.8546 - val_loss: 8.8167 - val_accuracy: 0.7703\n",
            "Epoch 315/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3177 - accuracy: 0.8558 - val_loss: 8.8054 - val_accuracy: 0.7751\n",
            "Epoch 316/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3178 - accuracy: 0.8546 - val_loss: 8.8124 - val_accuracy: 0.7751\n",
            "Epoch 317/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.8546 - val_loss: 8.8551 - val_accuracy: 0.7703\n",
            "Epoch 318/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3179 - accuracy: 0.8570 - val_loss: 8.8330 - val_accuracy: 0.7751\n",
            "Epoch 319/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.8546 - val_loss: 8.8307 - val_accuracy: 0.7751\n",
            "Epoch 320/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3177 - accuracy: 0.8570 - val_loss: 8.8285 - val_accuracy: 0.7751\n",
            "Epoch 321/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3176 - accuracy: 0.8546 - val_loss: 8.8326 - val_accuracy: 0.7751\n",
            "Epoch 322/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3176 - accuracy: 0.8558 - val_loss: 8.8459 - val_accuracy: 0.7751\n",
            "Epoch 323/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3211 - accuracy: 0.8558 - val_loss: 8.6730 - val_accuracy: 0.7703\n",
            "Epoch 324/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8558 - val_loss: 8.7119 - val_accuracy: 0.7703\n",
            "Epoch 325/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8558 - val_loss: 8.7383 - val_accuracy: 0.7703\n",
            "Epoch 326/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8546 - val_loss: 8.8061 - val_accuracy: 0.7751\n",
            "Epoch 327/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8570 - val_loss: 8.5566 - val_accuracy: 0.7751\n",
            "Epoch 328/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.8546 - val_loss: 8.6262 - val_accuracy: 0.7703\n",
            "Epoch 329/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3187 - accuracy: 0.8582 - val_loss: 8.6648 - val_accuracy: 0.7703\n",
            "Epoch 330/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.8570 - val_loss: 8.6873 - val_accuracy: 0.7703\n",
            "Epoch 331/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8558 - val_loss: 8.7065 - val_accuracy: 0.7703\n",
            "Epoch 332/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8570 - val_loss: 8.7257 - val_accuracy: 0.7703\n",
            "Epoch 333/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8582 - val_loss: 8.7313 - val_accuracy: 0.7703\n",
            "Epoch 334/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.8558 - val_loss: 8.7380 - val_accuracy: 0.7703\n",
            "Epoch 335/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8558 - val_loss: 8.7436 - val_accuracy: 0.7656\n",
            "Epoch 336/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3182 - accuracy: 0.8558 - val_loss: 8.7455 - val_accuracy: 0.7703\n",
            "Epoch 337/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3180 - accuracy: 0.8558 - val_loss: 8.7772 - val_accuracy: 0.7751\n",
            "Epoch 338/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.8570 - val_loss: 8.8077 - val_accuracy: 0.7751\n",
            "Epoch 339/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8558 - val_loss: 8.8445 - val_accuracy: 0.7703\n",
            "Epoch 340/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.8570 - val_loss: 8.8395 - val_accuracy: 0.7703\n",
            "Epoch 341/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8534 - val_loss: 8.8351 - val_accuracy: 0.7751\n",
            "Epoch 342/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8546 - val_loss: 8.8403 - val_accuracy: 0.7751\n",
            "Epoch 343/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8570 - val_loss: 8.8385 - val_accuracy: 0.7703\n",
            "Epoch 344/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8570 - val_loss: 8.8504 - val_accuracy: 0.7703\n",
            "Epoch 345/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8558 - val_loss: 8.8414 - val_accuracy: 0.7751\n",
            "Epoch 346/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3178 - accuracy: 0.8558 - val_loss: 8.8615 - val_accuracy: 0.7751\n",
            "Epoch 347/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8558 - val_loss: 8.8710 - val_accuracy: 0.7656\n",
            "Epoch 348/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8570 - val_loss: 8.8645 - val_accuracy: 0.7751\n",
            "Epoch 349/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.8610 - val_accuracy: 0.7751\n",
            "Epoch 350/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8558 - val_loss: 8.8622 - val_accuracy: 0.7751\n",
            "Epoch 351/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8570 - val_loss: 8.8678 - val_accuracy: 0.7751\n",
            "Epoch 352/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8558 - val_loss: 8.8736 - val_accuracy: 0.7751\n",
            "Epoch 353/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8582 - val_loss: 8.8716 - val_accuracy: 0.7751\n",
            "Epoch 354/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8582 - val_loss: 8.8739 - val_accuracy: 0.7751\n",
            "Epoch 355/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8558 - val_loss: 8.8867 - val_accuracy: 0.7751\n",
            "Epoch 356/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8570 - val_loss: 8.8747 - val_accuracy: 0.7751\n",
            "Epoch 357/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8570 - val_loss: 8.9106 - val_accuracy: 0.7703\n",
            "Epoch 358/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8570 - val_loss: 8.8837 - val_accuracy: 0.7751\n",
            "Epoch 359/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8534 - val_loss: 8.8978 - val_accuracy: 0.7751\n",
            "Epoch 360/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8546 - val_loss: 8.8957 - val_accuracy: 0.7751\n",
            "Epoch 361/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8546 - val_loss: 8.8885 - val_accuracy: 0.7751\n",
            "Epoch 362/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8546 - val_loss: 8.8751 - val_accuracy: 0.7751\n",
            "Epoch 363/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.8870 - val_accuracy: 0.7751\n",
            "Epoch 364/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.8906 - val_accuracy: 0.7751\n",
            "Epoch 365/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8558 - val_loss: 8.9190 - val_accuracy: 0.7751\n",
            "Epoch 366/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.8558 - val_loss: 8.8973 - val_accuracy: 0.7751\n",
            "Epoch 367/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8582 - val_loss: 8.9016 - val_accuracy: 0.7751\n",
            "Epoch 368/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3174 - accuracy: 0.8546 - val_loss: 8.8957 - val_accuracy: 0.7751\n",
            "Epoch 369/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.8987 - val_accuracy: 0.7751\n",
            "Epoch 370/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8546 - val_loss: 8.8938 - val_accuracy: 0.7751\n",
            "Epoch 371/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8534 - val_loss: 8.8812 - val_accuracy: 0.7751\n",
            "Epoch 372/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8558 - val_loss: 8.8909 - val_accuracy: 0.7751\n",
            "Epoch 373/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8546 - val_loss: 8.8900 - val_accuracy: 0.7751\n",
            "Epoch 374/1000\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8570 - val_loss: 8.9081 - val_accuracy: 0.7751\n",
            "Epoch 375/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8546 - val_loss: 8.9052 - val_accuracy: 0.7751\n",
            "Epoch 376/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8558 - val_loss: 8.9061 - val_accuracy: 0.7751\n",
            "Epoch 377/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.9030 - val_accuracy: 0.7751\n",
            "Epoch 378/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8546 - val_loss: 8.9144 - val_accuracy: 0.7751\n",
            "Epoch 379/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8558 - val_loss: 8.9156 - val_accuracy: 0.7751\n",
            "Epoch 380/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.9221 - val_accuracy: 0.7751\n",
            "Epoch 381/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8582 - val_loss: 8.8921 - val_accuracy: 0.7751\n",
            "Epoch 382/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.9238 - val_accuracy: 0.7751\n",
            "Epoch 383/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8558 - val_loss: 8.9066 - val_accuracy: 0.7751\n",
            "Epoch 384/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8570 - val_loss: 8.9215 - val_accuracy: 0.7751\n",
            "Epoch 385/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8582 - val_loss: 8.9233 - val_accuracy: 0.7751\n",
            "Epoch 386/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 8.9229 - val_accuracy: 0.7751\n",
            "Epoch 387/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3176 - accuracy: 0.8558 - val_loss: 8.9335 - val_accuracy: 0.7703\n",
            "Epoch 388/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8546 - val_loss: 8.9276 - val_accuracy: 0.7751\n",
            "Epoch 389/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8558 - val_loss: 8.9307 - val_accuracy: 0.7751\n",
            "Epoch 390/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8570 - val_loss: 8.9339 - val_accuracy: 0.7751\n",
            "Epoch 391/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8546 - val_loss: 8.9330 - val_accuracy: 0.7751\n",
            "Epoch 392/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8534 - val_loss: 8.9384 - val_accuracy: 0.7751\n",
            "Epoch 393/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8570 - val_loss: 8.9378 - val_accuracy: 0.7703\n",
            "Epoch 394/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8558 - val_loss: 8.9253 - val_accuracy: 0.7751\n",
            "Epoch 395/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3172 - accuracy: 0.8594 - val_loss: 8.9340 - val_accuracy: 0.7751\n",
            "Epoch 396/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8570 - val_loss: 8.9295 - val_accuracy: 0.7751\n",
            "Epoch 397/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 8.9351 - val_accuracy: 0.7751\n",
            "Epoch 398/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8546 - val_loss: 8.9311 - val_accuracy: 0.7751\n",
            "Epoch 399/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3171 - accuracy: 0.8570 - val_loss: 8.9379 - val_accuracy: 0.7751\n",
            "Epoch 400/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8594 - val_loss: 8.9431 - val_accuracy: 0.7751\n",
            "Epoch 401/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3173 - accuracy: 0.8570 - val_loss: 8.9604 - val_accuracy: 0.7703\n",
            "Epoch 402/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8570 - val_loss: 8.9507 - val_accuracy: 0.7751\n",
            "Epoch 403/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8570 - val_loss: 8.9445 - val_accuracy: 0.7751\n",
            "Epoch 404/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8582 - val_loss: 8.9516 - val_accuracy: 0.7751\n",
            "Epoch 405/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8570 - val_loss: 8.9343 - val_accuracy: 0.7751\n",
            "Epoch 406/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 8.9408 - val_accuracy: 0.7751\n",
            "Epoch 407/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3172 - accuracy: 0.8582 - val_loss: 8.9547 - val_accuracy: 0.7751\n",
            "Epoch 408/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3170 - accuracy: 0.8570 - val_loss: 8.9576 - val_accuracy: 0.7751\n",
            "Epoch 409/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3173 - accuracy: 0.8570 - val_loss: 8.9870 - val_accuracy: 0.7751\n",
            "Epoch 410/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3173 - accuracy: 0.8558 - val_loss: 8.9806 - val_accuracy: 0.7751\n",
            "Epoch 411/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3171 - accuracy: 0.8558 - val_loss: 8.9719 - val_accuracy: 0.7751\n",
            "Epoch 412/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3171 - accuracy: 0.8546 - val_loss: 8.9732 - val_accuracy: 0.7751\n",
            "Epoch 413/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3172 - accuracy: 0.8546 - val_loss: 8.9775 - val_accuracy: 0.7751\n",
            "Epoch 414/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3171 - accuracy: 0.8558 - val_loss: 8.9752 - val_accuracy: 0.7751\n",
            "Epoch 415/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 8.9887 - val_accuracy: 0.7751\n",
            "Epoch 416/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 8.9814 - val_accuracy: 0.7751\n",
            "Epoch 417/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 8.9840 - val_accuracy: 0.7751\n",
            "Epoch 418/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 8.9752 - val_accuracy: 0.7751\n",
            "Epoch 419/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3169 - accuracy: 0.8582 - val_loss: 8.9805 - val_accuracy: 0.7751\n",
            "Epoch 420/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3170 - accuracy: 0.8582 - val_loss: 8.9925 - val_accuracy: 0.7751\n",
            "Epoch 421/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8570 - val_loss: 8.9938 - val_accuracy: 0.7751\n",
            "Epoch 422/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 8.9858 - val_accuracy: 0.7751\n",
            "Epoch 423/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 9.0266 - val_accuracy: 0.7703\n",
            "Epoch 424/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3207 - accuracy: 0.8570 - val_loss: 8.7682 - val_accuracy: 0.7703\n",
            "Epoch 425/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3186 - accuracy: 0.8570 - val_loss: 8.8013 - val_accuracy: 0.7703\n",
            "Epoch 426/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3179 - accuracy: 0.8582 - val_loss: 8.8328 - val_accuracy: 0.7703\n",
            "Epoch 427/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.8570 - val_loss: 8.8471 - val_accuracy: 0.7703\n",
            "Epoch 428/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3176 - accuracy: 0.8570 - val_loss: 8.9217 - val_accuracy: 0.7751\n",
            "Epoch 429/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3171 - accuracy: 0.8582 - val_loss: 8.9267 - val_accuracy: 0.7703\n",
            "Epoch 430/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3174 - accuracy: 0.8546 - val_loss: 8.9644 - val_accuracy: 0.7751\n",
            "Epoch 431/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3173 - accuracy: 0.8546 - val_loss: 8.9531 - val_accuracy: 0.7751\n",
            "Epoch 432/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 8.9586 - val_accuracy: 0.7751\n",
            "Epoch 433/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3172 - accuracy: 0.8546 - val_loss: 8.9593 - val_accuracy: 0.7751\n",
            "Epoch 434/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3171 - accuracy: 0.8546 - val_loss: 8.9608 - val_accuracy: 0.7703\n",
            "Epoch 435/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 8.9707 - val_accuracy: 0.7703\n",
            "Epoch 436/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 8.9738 - val_accuracy: 0.7703\n",
            "Epoch 437/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8570 - val_loss: 8.9765 - val_accuracy: 0.7703\n",
            "Epoch 438/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.8570 - val_loss: 8.9762 - val_accuracy: 0.7656\n",
            "Epoch 439/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 9.0115 - val_accuracy: 0.7751\n",
            "Epoch 440/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.8558 - val_loss: 8.9847 - val_accuracy: 0.7703\n",
            "Epoch 441/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3171 - accuracy: 0.8558 - val_loss: 8.9777 - val_accuracy: 0.7703\n",
            "Epoch 442/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.8546 - val_loss: 8.9802 - val_accuracy: 0.7703\n",
            "Epoch 443/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 8.9889 - val_accuracy: 0.7703\n",
            "Epoch 444/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8570 - val_loss: 8.9810 - val_accuracy: 0.7703\n",
            "Epoch 445/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8558 - val_loss: 8.9853 - val_accuracy: 0.7751\n",
            "Epoch 446/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8558 - val_loss: 8.9900 - val_accuracy: 0.7751\n",
            "Epoch 447/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.8546 - val_loss: 8.9948 - val_accuracy: 0.7751\n",
            "Epoch 448/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8558 - val_loss: 9.0253 - val_accuracy: 0.7751\n",
            "Epoch 449/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8546 - val_loss: 9.0218 - val_accuracy: 0.7751\n",
            "Epoch 450/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3171 - accuracy: 0.8546 - val_loss: 9.0146 - val_accuracy: 0.7751\n",
            "Epoch 451/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.8558 - val_loss: 9.0153 - val_accuracy: 0.7751\n",
            "Epoch 452/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 9.0169 - val_accuracy: 0.7751\n",
            "Epoch 453/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8558 - val_loss: 9.0057 - val_accuracy: 0.7751\n",
            "Epoch 454/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 9.0383 - val_accuracy: 0.7751\n",
            "Epoch 455/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8558 - val_loss: 9.0263 - val_accuracy: 0.7751\n",
            "Epoch 456/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8570 - val_loss: 9.0298 - val_accuracy: 0.7751\n",
            "Epoch 457/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3169 - accuracy: 0.8558 - val_loss: 9.0302 - val_accuracy: 0.7751\n",
            "Epoch 458/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8558 - val_loss: 9.0264 - val_accuracy: 0.7751\n",
            "Epoch 459/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8582 - val_loss: 9.0629 - val_accuracy: 0.7751\n",
            "Epoch 460/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8582 - val_loss: 9.0434 - val_accuracy: 0.7751\n",
            "Epoch 461/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8570 - val_loss: 9.0090 - val_accuracy: 0.7751\n",
            "Epoch 462/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8582 - val_loss: 9.0078 - val_accuracy: 0.7751\n",
            "Epoch 463/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3169 - accuracy: 0.8558 - val_loss: 9.0135 - val_accuracy: 0.7751\n",
            "Epoch 464/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8558 - val_loss: 9.0177 - val_accuracy: 0.7751\n",
            "Epoch 465/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8558 - val_loss: 9.0212 - val_accuracy: 0.7751\n",
            "Epoch 466/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8558 - val_loss: 9.0327 - val_accuracy: 0.7751\n",
            "Epoch 467/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8570 - val_loss: 9.0319 - val_accuracy: 0.7751\n",
            "Epoch 468/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8546 - val_loss: 9.0314 - val_accuracy: 0.7751\n",
            "Epoch 469/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3166 - accuracy: 0.8546 - val_loss: 9.0312 - val_accuracy: 0.7751\n",
            "Epoch 470/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8582 - val_loss: 9.0476 - val_accuracy: 0.7751\n",
            "Epoch 471/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8570 - val_loss: 9.0499 - val_accuracy: 0.7751\n",
            "Epoch 472/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0541 - val_accuracy: 0.7751\n",
            "Epoch 473/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0335 - val_accuracy: 0.7751\n",
            "Epoch 474/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3165 - accuracy: 0.8570 - val_loss: 9.0805 - val_accuracy: 0.7751\n",
            "Epoch 475/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8570 - val_loss: 9.0595 - val_accuracy: 0.7751\n",
            "Epoch 476/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0642 - val_accuracy: 0.7751\n",
            "Epoch 477/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8546 - val_loss: 9.0657 - val_accuracy: 0.7751\n",
            "Epoch 478/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8546 - val_loss: 9.0538 - val_accuracy: 0.7751\n",
            "Epoch 479/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8558 - val_loss: 8.9843 - val_accuracy: 0.7703\n",
            "Epoch 480/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8546 - val_loss: 8.9978 - val_accuracy: 0.7751\n",
            "Epoch 481/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8570 - val_loss: 9.0065 - val_accuracy: 0.7751\n",
            "Epoch 482/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8570 - val_loss: 9.0233 - val_accuracy: 0.7751\n",
            "Epoch 483/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8546 - val_loss: 9.0331 - val_accuracy: 0.7751\n",
            "Epoch 484/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0390 - val_accuracy: 0.7751\n",
            "Epoch 485/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8546 - val_loss: 9.0474 - val_accuracy: 0.7751\n",
            "Epoch 486/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3167 - accuracy: 0.8582 - val_loss: 9.0814 - val_accuracy: 0.7751\n",
            "Epoch 487/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 9.0583 - val_accuracy: 0.7751\n",
            "Epoch 488/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0559 - val_accuracy: 0.7751\n",
            "Epoch 489/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8546 - val_loss: 9.0511 - val_accuracy: 0.7751\n",
            "Epoch 490/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3168 - accuracy: 0.8558 - val_loss: 9.0517 - val_accuracy: 0.7751\n",
            "Epoch 491/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0591 - val_accuracy: 0.7751\n",
            "Epoch 492/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0624 - val_accuracy: 0.7751\n",
            "Epoch 493/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8546 - val_loss: 9.0686 - val_accuracy: 0.7751\n",
            "Epoch 494/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0718 - val_accuracy: 0.7751\n",
            "Epoch 495/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.8558 - val_loss: 9.0831 - val_accuracy: 0.7751\n",
            "Epoch 496/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8546 - val_loss: 9.1139 - val_accuracy: 0.7751\n",
            "Epoch 497/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8558 - val_loss: 9.0962 - val_accuracy: 0.7751\n",
            "Epoch 498/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8558 - val_loss: 9.0859 - val_accuracy: 0.7751\n",
            "Epoch 499/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8558 - val_loss: 9.0874 - val_accuracy: 0.7703\n",
            "Epoch 500/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8558 - val_loss: 9.0811 - val_accuracy: 0.7799\n",
            "Epoch 501/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3164 - accuracy: 0.8558 - val_loss: 9.0710 - val_accuracy: 0.7703\n",
            "Epoch 502/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8570 - val_loss: 9.0835 - val_accuracy: 0.7703\n",
            "Epoch 503/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8546 - val_loss: 9.1029 - val_accuracy: 0.7751\n",
            "Epoch 504/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3166 - accuracy: 0.8570 - val_loss: 9.0914 - val_accuracy: 0.7751\n",
            "Epoch 505/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.0972 - val_accuracy: 0.7751\n",
            "Epoch 506/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3165 - accuracy: 0.8558 - val_loss: 9.0913 - val_accuracy: 0.7751\n",
            "Epoch 507/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.8558 - val_loss: 9.0916 - val_accuracy: 0.7703\n",
            "Epoch 508/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3164 - accuracy: 0.8558 - val_loss: 9.0914 - val_accuracy: 0.7703\n",
            "Epoch 509/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3165 - accuracy: 0.8570 - val_loss: 9.0982 - val_accuracy: 0.7751\n",
            "Epoch 510/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8546 - val_loss: 9.0955 - val_accuracy: 0.7751\n",
            "Epoch 511/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8558 - val_loss: 9.0987 - val_accuracy: 0.7751\n",
            "Epoch 512/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3164 - accuracy: 0.8558 - val_loss: 9.0963 - val_accuracy: 0.7751\n",
            "Epoch 513/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3170 - accuracy: 0.8558 - val_loss: 9.1199 - val_accuracy: 0.7751\n",
            "Epoch 514/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.1090 - val_accuracy: 0.7751\n",
            "Epoch 515/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3164 - accuracy: 0.8546 - val_loss: 9.1256 - val_accuracy: 0.7799\n",
            "Epoch 516/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8546 - val_loss: 9.1250 - val_accuracy: 0.7751\n",
            "Epoch 517/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8558 - val_loss: 9.1326 - val_accuracy: 0.7751\n",
            "Epoch 518/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3166 - accuracy: 0.8546 - val_loss: 9.1338 - val_accuracy: 0.7751\n",
            "Epoch 519/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3165 - accuracy: 0.8558 - val_loss: 9.1302 - val_accuracy: 0.7751\n",
            "Epoch 520/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3164 - accuracy: 0.8546 - val_loss: 9.1299 - val_accuracy: 0.7751\n",
            "Epoch 521/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3166 - accuracy: 0.8546 - val_loss: 9.1295 - val_accuracy: 0.7703\n",
            "Epoch 522/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3163 - accuracy: 0.8582 - val_loss: 9.1348 - val_accuracy: 0.7751\n",
            "Epoch 523/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3162 - accuracy: 0.8558 - val_loss: 9.1505 - val_accuracy: 0.7751\n",
            "Epoch 524/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3157 - accuracy: 0.8546 - val_loss: 9.1556 - val_accuracy: 0.7751\n",
            "Epoch 525/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3165 - accuracy: 0.8546 - val_loss: 9.1694 - val_accuracy: 0.7751\n",
            "Epoch 526/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3161 - accuracy: 0.8546 - val_loss: 9.0484 - val_accuracy: 0.7703\n",
            "Epoch 527/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3160 - accuracy: 0.8558 - val_loss: 9.0832 - val_accuracy: 0.7751\n",
            "Epoch 528/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.8558 - val_loss: 9.1232 - val_accuracy: 0.7751\n",
            "Epoch 529/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3161 - accuracy: 0.8558 - val_loss: 9.1354 - val_accuracy: 0.7751\n",
            "Epoch 530/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8558 - val_loss: 9.1366 - val_accuracy: 0.7751\n",
            "Epoch 531/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3160 - accuracy: 0.8546 - val_loss: 9.1359 - val_accuracy: 0.7751\n",
            "Epoch 532/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8546 - val_loss: 9.1541 - val_accuracy: 0.7751\n",
            "Epoch 533/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8594 - val_loss: 9.1562 - val_accuracy: 0.7751\n",
            "Epoch 534/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3158 - accuracy: 0.8570 - val_loss: 9.1584 - val_accuracy: 0.7751\n",
            "Epoch 535/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3157 - accuracy: 0.8546 - val_loss: 9.1745 - val_accuracy: 0.7751\n",
            "Epoch 536/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3157 - accuracy: 0.8546 - val_loss: 9.1817 - val_accuracy: 0.7703\n",
            "Epoch 537/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3162 - accuracy: 0.8546 - val_loss: 9.2112 - val_accuracy: 0.7751\n",
            "Epoch 538/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3160 - accuracy: 0.8546 - val_loss: 9.1888 - val_accuracy: 0.7751\n",
            "Epoch 539/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3157 - accuracy: 0.8534 - val_loss: 9.1825 - val_accuracy: 0.7799\n",
            "Epoch 540/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8558 - val_loss: 9.1792 - val_accuracy: 0.7751\n",
            "Epoch 541/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3158 - accuracy: 0.8558 - val_loss: 9.1782 - val_accuracy: 0.7751\n",
            "Epoch 542/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3157 - accuracy: 0.8546 - val_loss: 9.1830 - val_accuracy: 0.7751\n",
            "Epoch 543/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8546 - val_loss: 9.2247 - val_accuracy: 0.7751\n",
            "Epoch 544/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3158 - accuracy: 0.8558 - val_loss: 9.2197 - val_accuracy: 0.7751\n",
            "Epoch 545/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3166 - accuracy: 0.8558 - val_loss: 9.1382 - val_accuracy: 0.7703\n",
            "Epoch 546/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3159 - accuracy: 0.8546 - val_loss: 9.1546 - val_accuracy: 0.7703\n",
            "Epoch 547/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3159 - accuracy: 0.8558 - val_loss: 9.1599 - val_accuracy: 0.7751\n",
            "Epoch 548/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3156 - accuracy: 0.8546 - val_loss: 9.1769 - val_accuracy: 0.7751\n",
            "Epoch 549/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3157 - accuracy: 0.8582 - val_loss: 9.1689 - val_accuracy: 0.7751\n",
            "Epoch 550/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3156 - accuracy: 0.8558 - val_loss: 9.1841 - val_accuracy: 0.7751\n",
            "Epoch 551/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8558 - val_loss: 9.1858 - val_accuracy: 0.7799\n",
            "Epoch 552/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3159 - accuracy: 0.8582 - val_loss: 9.2168 - val_accuracy: 0.7799\n",
            "Epoch 553/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8534 - val_loss: 9.0164 - val_accuracy: 0.7751\n",
            "Epoch 554/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8570 - val_loss: 9.0485 - val_accuracy: 0.7751\n",
            "Epoch 555/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3163 - accuracy: 0.8570 - val_loss: 9.1004 - val_accuracy: 0.7751\n",
            "Epoch 556/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.8558 - val_loss: 9.0889 - val_accuracy: 0.7751\n",
            "Epoch 557/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3159 - accuracy: 0.8570 - val_loss: 9.1495 - val_accuracy: 0.7751\n",
            "Epoch 558/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3159 - accuracy: 0.8570 - val_loss: 9.1601 - val_accuracy: 0.7799\n",
            "Epoch 559/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8546 - val_loss: 9.1665 - val_accuracy: 0.7799\n",
            "Epoch 560/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8558 - val_loss: 9.1775 - val_accuracy: 0.7799\n",
            "Epoch 561/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8546 - val_loss: 9.1840 - val_accuracy: 0.7799\n",
            "Epoch 562/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8570 - val_loss: 9.1932 - val_accuracy: 0.7799\n",
            "Epoch 563/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8570 - val_loss: 9.1925 - val_accuracy: 0.7799\n",
            "Epoch 564/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8558 - val_loss: 9.1927 - val_accuracy: 0.7799\n",
            "Epoch 565/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.8546 - val_loss: 9.1983 - val_accuracy: 0.7799\n",
            "Epoch 566/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3158 - accuracy: 0.8558 - val_loss: 9.2299 - val_accuracy: 0.7799\n",
            "Epoch 567/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3159 - accuracy: 0.8570 - val_loss: 9.2129 - val_accuracy: 0.7799\n",
            "Epoch 568/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8546 - val_loss: 9.2183 - val_accuracy: 0.7799\n",
            "Epoch 569/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8546 - val_loss: 9.2154 - val_accuracy: 0.7799\n",
            "Epoch 570/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3157 - accuracy: 0.8558 - val_loss: 9.2194 - val_accuracy: 0.7799\n",
            "Epoch 571/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.8558 - val_loss: 9.2280 - val_accuracy: 0.7799\n",
            "Epoch 572/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8570 - val_loss: 9.2215 - val_accuracy: 0.7799\n",
            "Epoch 573/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8558 - val_loss: 9.2266 - val_accuracy: 0.7799\n",
            "Epoch 574/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3154 - accuracy: 0.8546 - val_loss: 9.2419 - val_accuracy: 0.7799\n",
            "Epoch 575/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8558 - val_loss: 9.2318 - val_accuracy: 0.7799\n",
            "Epoch 576/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8558 - val_loss: 9.2739 - val_accuracy: 0.7799\n",
            "Epoch 577/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3157 - accuracy: 0.8558 - val_loss: 9.2777 - val_accuracy: 0.7799\n",
            "Epoch 578/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8546 - val_loss: 9.2618 - val_accuracy: 0.7799\n",
            "Epoch 579/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.8546 - val_loss: 9.2599 - val_accuracy: 0.7799\n",
            "Epoch 580/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8546 - val_loss: 9.2525 - val_accuracy: 0.7799\n",
            "Epoch 581/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3154 - accuracy: 0.8546 - val_loss: 9.2588 - val_accuracy: 0.7799\n",
            "Epoch 582/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8558 - val_loss: 9.2596 - val_accuracy: 0.7799\n",
            "Epoch 583/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2547 - val_accuracy: 0.7799\n",
            "Epoch 584/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8546 - val_loss: 9.2503 - val_accuracy: 0.7799\n",
            "Epoch 585/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8558 - val_loss: 9.2584 - val_accuracy: 0.7799\n",
            "Epoch 586/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3154 - accuracy: 0.8558 - val_loss: 9.2554 - val_accuracy: 0.7799\n",
            "Epoch 587/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2596 - val_accuracy: 0.7703\n",
            "Epoch 588/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8570 - val_loss: 9.2859 - val_accuracy: 0.7799\n",
            "Epoch 589/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8558 - val_loss: 9.2669 - val_accuracy: 0.7799\n",
            "Epoch 590/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2809 - val_accuracy: 0.7799\n",
            "Epoch 591/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2694 - val_accuracy: 0.7799\n",
            "Epoch 592/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8570 - val_loss: 9.2779 - val_accuracy: 0.7799\n",
            "Epoch 593/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8558 - val_loss: 9.2759 - val_accuracy: 0.7799\n",
            "Epoch 594/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.8558 - val_loss: 9.1766 - val_accuracy: 0.7751\n",
            "Epoch 595/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8546 - val_loss: 9.2116 - val_accuracy: 0.7799\n",
            "Epoch 596/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8558 - val_loss: 9.2265 - val_accuracy: 0.7799\n",
            "Epoch 597/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3156 - accuracy: 0.8558 - val_loss: 9.2615 - val_accuracy: 0.7799\n",
            "Epoch 598/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.8546 - val_loss: 9.2688 - val_accuracy: 0.7799\n",
            "Epoch 599/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8546 - val_loss: 9.2613 - val_accuracy: 0.7799\n",
            "Epoch 600/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2766 - val_accuracy: 0.7799\n",
            "Epoch 601/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3152 - accuracy: 0.8558 - val_loss: 9.2688 - val_accuracy: 0.7799\n",
            "Epoch 602/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2757 - val_accuracy: 0.7799\n",
            "Epoch 603/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3152 - accuracy: 0.8558 - val_loss: 9.2721 - val_accuracy: 0.7799\n",
            "Epoch 604/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8534 - val_loss: 9.3039 - val_accuracy: 0.7799\n",
            "Epoch 605/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2954 - val_accuracy: 0.7799\n",
            "Epoch 606/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.2974 - val_accuracy: 0.7799\n",
            "Epoch 607/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.3012 - val_accuracy: 0.7799\n",
            "Epoch 608/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3153 - accuracy: 0.8558 - val_loss: 9.3047 - val_accuracy: 0.7799\n",
            "Epoch 609/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8546 - val_loss: 9.3164 - val_accuracy: 0.7799\n",
            "Epoch 610/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8558 - val_loss: 9.3050 - val_accuracy: 0.7847\n",
            "Epoch 611/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3152 - accuracy: 0.8570 - val_loss: 9.3075 - val_accuracy: 0.7799\n",
            "Epoch 612/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8558 - val_loss: 9.3080 - val_accuracy: 0.7799\n",
            "Epoch 613/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.2939 - val_accuracy: 0.7799\n",
            "Epoch 614/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3151 - accuracy: 0.8558 - val_loss: 9.2912 - val_accuracy: 0.7799\n",
            "Epoch 615/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3152 - accuracy: 0.8570 - val_loss: 9.2992 - val_accuracy: 0.7799\n",
            "Epoch 616/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3153 - accuracy: 0.8570 - val_loss: 9.3398 - val_accuracy: 0.7799\n",
            "Epoch 617/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8570 - val_loss: 9.3245 - val_accuracy: 0.7799\n",
            "Epoch 618/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3152 - accuracy: 0.8558 - val_loss: 9.3231 - val_accuracy: 0.7799\n",
            "Epoch 619/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.3258 - val_accuracy: 0.7799\n",
            "Epoch 620/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3150 - accuracy: 0.8546 - val_loss: 9.3247 - val_accuracy: 0.7799\n",
            "Epoch 621/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.3188 - val_accuracy: 0.7799\n",
            "Epoch 622/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3151 - accuracy: 0.8558 - val_loss: 9.3341 - val_accuracy: 0.7799\n",
            "Epoch 623/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3151 - accuracy: 0.8546 - val_loss: 9.3339 - val_accuracy: 0.7799\n",
            "Epoch 624/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8558 - val_loss: 9.2094 - val_accuracy: 0.7751\n",
            "Epoch 625/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8570 - val_loss: 9.2629 - val_accuracy: 0.7799\n",
            "Epoch 626/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8546 - val_loss: 9.2852 - val_accuracy: 0.7799\n",
            "Epoch 627/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8558 - val_loss: 9.2990 - val_accuracy: 0.7799\n",
            "Epoch 628/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8570 - val_loss: 9.3406 - val_accuracy: 0.7799\n",
            "Epoch 629/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3154 - accuracy: 0.8570 - val_loss: 9.3257 - val_accuracy: 0.7799\n",
            "Epoch 630/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.3178 - val_accuracy: 0.7799\n",
            "Epoch 631/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3151 - accuracy: 0.8570 - val_loss: 9.3147 - val_accuracy: 0.7799\n",
            "Epoch 632/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3151 - accuracy: 0.8546 - val_loss: 9.3173 - val_accuracy: 0.7799\n",
            "Epoch 633/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3150 - accuracy: 0.8558 - val_loss: 9.3303 - val_accuracy: 0.7799\n",
            "Epoch 634/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8534 - val_loss: 9.3378 - val_accuracy: 0.7799\n",
            "Epoch 635/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3149 - accuracy: 0.8558 - val_loss: 9.3365 - val_accuracy: 0.7847\n",
            "Epoch 636/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3149 - accuracy: 0.8570 - val_loss: 9.3328 - val_accuracy: 0.7799\n",
            "Epoch 637/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.3242 - val_accuracy: 0.7799\n",
            "Epoch 638/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8558 - val_loss: 9.3474 - val_accuracy: 0.7799\n",
            "Epoch 639/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.3414 - val_accuracy: 0.7799\n",
            "Epoch 640/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3149 - accuracy: 0.8570 - val_loss: 9.3473 - val_accuracy: 0.7799\n",
            "Epoch 641/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8558 - val_loss: 9.3512 - val_accuracy: 0.7799\n",
            "Epoch 642/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8558 - val_loss: 9.3604 - val_accuracy: 0.7799\n",
            "Epoch 643/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3150 - accuracy: 0.8546 - val_loss: 9.3102 - val_accuracy: 0.7847\n",
            "Epoch 644/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.3135 - val_accuracy: 0.7799\n",
            "Epoch 645/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3149 - accuracy: 0.8558 - val_loss: 9.3132 - val_accuracy: 0.7799\n",
            "Epoch 646/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8558 - val_loss: 9.3251 - val_accuracy: 0.7799\n",
            "Epoch 647/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3148 - accuracy: 0.8546 - val_loss: 9.3378 - val_accuracy: 0.7799\n",
            "Epoch 648/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3149 - accuracy: 0.8546 - val_loss: 9.3405 - val_accuracy: 0.7799\n",
            "Epoch 649/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3150 - accuracy: 0.8546 - val_loss: 9.3755 - val_accuracy: 0.7751\n",
            "Epoch 650/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.3627 - val_accuracy: 0.7799\n",
            "Epoch 651/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3147 - accuracy: 0.8570 - val_loss: 9.3648 - val_accuracy: 0.7799\n",
            "Epoch 652/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3149 - accuracy: 0.8570 - val_loss: 9.3633 - val_accuracy: 0.7799\n",
            "Epoch 653/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3155 - accuracy: 0.8570 - val_loss: 9.3053 - val_accuracy: 0.7799\n",
            "Epoch 654/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3154 - accuracy: 0.8558 - val_loss: 9.3250 - val_accuracy: 0.7799\n",
            "Epoch 655/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3155 - accuracy: 0.8570 - val_loss: 9.3555 - val_accuracy: 0.7799\n",
            "Epoch 656/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3150 - accuracy: 0.8558 - val_loss: 9.3481 - val_accuracy: 0.7799\n",
            "Epoch 657/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.3440 - val_accuracy: 0.7799\n",
            "Epoch 658/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3147 - accuracy: 0.8570 - val_loss: 9.3512 - val_accuracy: 0.7799\n",
            "Epoch 659/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3148 - accuracy: 0.8582 - val_loss: 9.3518 - val_accuracy: 0.7799\n",
            "Epoch 660/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.3643 - val_accuracy: 0.7799\n",
            "Epoch 661/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.4052 - val_accuracy: 0.7799\n",
            "Epoch 662/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3150 - accuracy: 0.8546 - val_loss: 9.3817 - val_accuracy: 0.7799\n",
            "Epoch 663/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.3806 - val_accuracy: 0.7799\n",
            "Epoch 664/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8546 - val_loss: 9.3735 - val_accuracy: 0.7799\n",
            "Epoch 665/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.8546 - val_loss: 9.3813 - val_accuracy: 0.7799\n",
            "Epoch 666/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.3845 - val_accuracy: 0.7799\n",
            "Epoch 667/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8582 - val_loss: 9.4233 - val_accuracy: 0.7799\n",
            "Epoch 668/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.4080 - val_accuracy: 0.7799\n",
            "Epoch 669/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.4064 - val_accuracy: 0.7799\n",
            "Epoch 670/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.8558 - val_loss: 9.4016 - val_accuracy: 0.7799\n",
            "Epoch 671/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8558 - val_loss: 9.4040 - val_accuracy: 0.7799\n",
            "Epoch 672/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3147 - accuracy: 0.8558 - val_loss: 9.4087 - val_accuracy: 0.7847\n",
            "Epoch 673/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3149 - accuracy: 0.8570 - val_loss: 9.4073 - val_accuracy: 0.7847\n",
            "Epoch 674/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3147 - accuracy: 0.8558 - val_loss: 9.4086 - val_accuracy: 0.7799\n",
            "Epoch 675/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.8558 - val_loss: 9.4116 - val_accuracy: 0.7799\n",
            "Epoch 676/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8570 - val_loss: 9.4185 - val_accuracy: 0.7799\n",
            "Epoch 677/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8558 - val_loss: 9.4195 - val_accuracy: 0.7799\n",
            "Epoch 678/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3147 - accuracy: 0.8558 - val_loss: 9.4176 - val_accuracy: 0.7799\n",
            "Epoch 679/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3147 - accuracy: 0.8546 - val_loss: 9.4187 - val_accuracy: 0.7799\n",
            "Epoch 680/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.8558 - val_loss: 9.4174 - val_accuracy: 0.7799\n",
            "Epoch 681/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.8570 - val_loss: 9.4076 - val_accuracy: 0.7799\n",
            "Epoch 682/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3145 - accuracy: 0.8546 - val_loss: 9.4158 - val_accuracy: 0.7847\n",
            "Epoch 683/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8582 - val_loss: 9.4216 - val_accuracy: 0.7799\n",
            "Epoch 684/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8570 - val_loss: 9.4251 - val_accuracy: 0.7799\n",
            "Epoch 685/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8570 - val_loss: 9.4103 - val_accuracy: 0.7799\n",
            "Epoch 686/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3149 - accuracy: 0.8558 - val_loss: 9.4476 - val_accuracy: 0.7799\n",
            "Epoch 687/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.4369 - val_accuracy: 0.7799\n",
            "Epoch 688/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3147 - accuracy: 0.8582 - val_loss: 9.4336 - val_accuracy: 0.7799\n",
            "Epoch 689/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8558 - val_loss: 9.4329 - val_accuracy: 0.7799\n",
            "Epoch 690/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.4305 - val_accuracy: 0.7799\n",
            "Epoch 691/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8582 - val_loss: 9.4328 - val_accuracy: 0.7799\n",
            "Epoch 692/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3144 - accuracy: 0.8570 - val_loss: 9.4405 - val_accuracy: 0.7799\n",
            "Epoch 693/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.4412 - val_accuracy: 0.7799\n",
            "Epoch 694/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8558 - val_loss: 9.4432 - val_accuracy: 0.7799\n",
            "Epoch 695/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8558 - val_loss: 9.4374 - val_accuracy: 0.7799\n",
            "Epoch 696/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3147 - accuracy: 0.8558 - val_loss: 9.4774 - val_accuracy: 0.7751\n",
            "Epoch 697/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.4309 - val_accuracy: 0.7799\n",
            "Epoch 698/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3154 - accuracy: 0.8546 - val_loss: 9.4017 - val_accuracy: 0.7751\n",
            "Epoch 699/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3151 - accuracy: 0.8570 - val_loss: 9.4164 - val_accuracy: 0.7751\n",
            "Epoch 700/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3150 - accuracy: 0.8570 - val_loss: 9.4238 - val_accuracy: 0.7799\n",
            "Epoch 701/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3146 - accuracy: 0.8558 - val_loss: 9.4346 - val_accuracy: 0.7799\n",
            "Epoch 702/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.4328 - val_accuracy: 0.7847\n",
            "Epoch 703/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8582 - val_loss: 9.4373 - val_accuracy: 0.7847\n",
            "Epoch 704/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3148 - accuracy: 0.8582 - val_loss: 9.4651 - val_accuracy: 0.7799\n",
            "Epoch 705/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8582 - val_loss: 9.3152 - val_accuracy: 0.7799\n",
            "Epoch 706/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8582 - val_loss: 9.3320 - val_accuracy: 0.7751\n",
            "Epoch 707/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 9.3835 - val_accuracy: 0.7799\n",
            "Epoch 708/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8582 - val_loss: 9.3933 - val_accuracy: 0.7799\n",
            "Epoch 709/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.4097 - val_accuracy: 0.7799\n",
            "Epoch 710/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8558 - val_loss: 9.4147 - val_accuracy: 0.7799\n",
            "Epoch 711/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8594 - val_loss: 9.4183 - val_accuracy: 0.7799\n",
            "Epoch 712/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8570 - val_loss: 9.4310 - val_accuracy: 0.7799\n",
            "Epoch 713/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8546 - val_loss: 9.4321 - val_accuracy: 0.7799\n",
            "Epoch 714/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8570 - val_loss: 9.4283 - val_accuracy: 0.7799\n",
            "Epoch 715/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8546 - val_loss: 9.4344 - val_accuracy: 0.7799\n",
            "Epoch 716/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.4726 - val_accuracy: 0.7799\n",
            "Epoch 717/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.4721 - val_accuracy: 0.7799\n",
            "Epoch 718/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3146 - accuracy: 0.8570 - val_loss: 9.4738 - val_accuracy: 0.7799\n",
            "Epoch 719/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8582 - val_loss: 9.4624 - val_accuracy: 0.7799\n",
            "Epoch 720/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8570 - val_loss: 9.4696 - val_accuracy: 0.7799\n",
            "Epoch 721/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8570 - val_loss: 9.4681 - val_accuracy: 0.7799\n",
            "Epoch 722/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.4837 - val_accuracy: 0.7799\n",
            "Epoch 723/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8582 - val_loss: 9.4767 - val_accuracy: 0.7799\n",
            "Epoch 724/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.4737 - val_accuracy: 0.7799\n",
            "Epoch 725/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3144 - accuracy: 0.8582 - val_loss: 9.5114 - val_accuracy: 0.7847\n",
            "Epoch 726/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8570 - val_loss: 9.4890 - val_accuracy: 0.7799\n",
            "Epoch 727/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.4911 - val_accuracy: 0.7799\n",
            "Epoch 728/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8582 - val_loss: 9.4869 - val_accuracy: 0.7799\n",
            "Epoch 729/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.4907 - val_accuracy: 0.7799\n",
            "Epoch 730/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.4827 - val_accuracy: 0.7799\n",
            "Epoch 731/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3144 - accuracy: 0.8570 - val_loss: 9.5291 - val_accuracy: 0.7799\n",
            "Epoch 732/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.5137 - val_accuracy: 0.7799\n",
            "Epoch 733/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3143 - accuracy: 0.8582 - val_loss: 9.5154 - val_accuracy: 0.7799\n",
            "Epoch 734/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5101 - val_accuracy: 0.7799\n",
            "Epoch 735/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5122 - val_accuracy: 0.7799\n",
            "Epoch 736/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5111 - val_accuracy: 0.7799\n",
            "Epoch 737/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3141 - accuracy: 0.8546 - val_loss: 9.5003 - val_accuracy: 0.7799\n",
            "Epoch 738/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5006 - val_accuracy: 0.7799\n",
            "Epoch 739/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5099 - val_accuracy: 0.7799\n",
            "Epoch 740/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3140 - accuracy: 0.8594 - val_loss: 9.5216 - val_accuracy: 0.7847\n",
            "Epoch 741/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.5150 - val_accuracy: 0.7799\n",
            "Epoch 742/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3142 - accuracy: 0.8582 - val_loss: 9.5510 - val_accuracy: 0.7799\n",
            "Epoch 743/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3144 - accuracy: 0.8594 - val_loss: 9.5434 - val_accuracy: 0.7799\n",
            "Epoch 744/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3143 - accuracy: 0.8570 - val_loss: 9.5411 - val_accuracy: 0.7799\n",
            "Epoch 745/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3142 - accuracy: 0.8558 - val_loss: 9.5365 - val_accuracy: 0.7799\n",
            "Epoch 746/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5405 - val_accuracy: 0.7799\n",
            "Epoch 747/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5399 - val_accuracy: 0.7799\n",
            "Epoch 748/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3144 - accuracy: 0.8570 - val_loss: 9.3880 - val_accuracy: 0.7751\n",
            "Epoch 749/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3145 - accuracy: 0.8582 - val_loss: 9.4603 - val_accuracy: 0.7799\n",
            "Epoch 750/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.4860 - val_accuracy: 0.7799\n",
            "Epoch 751/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.4911 - val_accuracy: 0.7799\n",
            "Epoch 752/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5038 - val_accuracy: 0.7799\n",
            "Epoch 753/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3144 - accuracy: 0.8582 - val_loss: 9.5261 - val_accuracy: 0.7799\n",
            "Epoch 754/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5246 - val_accuracy: 0.7799\n",
            "Epoch 755/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5308 - val_accuracy: 0.7799\n",
            "Epoch 756/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.5318 - val_accuracy: 0.7799\n",
            "Epoch 757/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5285 - val_accuracy: 0.7799\n",
            "Epoch 758/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5318 - val_accuracy: 0.7799\n",
            "Epoch 759/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3139 - accuracy: 0.8570 - val_loss: 9.5141 - val_accuracy: 0.7799\n",
            "Epoch 760/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3143 - accuracy: 0.8570 - val_loss: 9.5418 - val_accuracy: 0.7799\n",
            "Epoch 761/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5387 - val_accuracy: 0.7799\n",
            "Epoch 762/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5432 - val_accuracy: 0.7847\n",
            "Epoch 763/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3143 - accuracy: 0.8582 - val_loss: 9.5391 - val_accuracy: 0.7799\n",
            "Epoch 764/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5396 - val_accuracy: 0.7799\n",
            "Epoch 765/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5303 - val_accuracy: 0.7799\n",
            "Epoch 766/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8558 - val_loss: 9.5390 - val_accuracy: 0.7799\n",
            "Epoch 767/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8582 - val_loss: 9.5418 - val_accuracy: 0.7799\n",
            "Epoch 768/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8594 - val_loss: 9.5500 - val_accuracy: 0.7847\n",
            "Epoch 769/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8558 - val_loss: 9.5436 - val_accuracy: 0.7799\n",
            "Epoch 770/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8558 - val_loss: 9.5476 - val_accuracy: 0.7799\n",
            "Epoch 771/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.5614 - val_accuracy: 0.7847\n",
            "Epoch 772/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5613 - val_accuracy: 0.7847\n",
            "Epoch 773/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3140 - accuracy: 0.8558 - val_loss: 9.5574 - val_accuracy: 0.7799\n",
            "Epoch 774/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5564 - val_accuracy: 0.7799\n",
            "Epoch 775/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8570 - val_loss: 9.5515 - val_accuracy: 0.7799\n",
            "Epoch 776/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8570 - val_loss: 9.5598 - val_accuracy: 0.7847\n",
            "Epoch 777/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5591 - val_accuracy: 0.7799\n",
            "Epoch 778/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.8570 - val_loss: 9.5842 - val_accuracy: 0.7799\n",
            "Epoch 779/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5850 - val_accuracy: 0.7799\n",
            "Epoch 780/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.5833 - val_accuracy: 0.7799\n",
            "Epoch 781/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8582 - val_loss: 9.5842 - val_accuracy: 0.7799\n",
            "Epoch 782/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5899 - val_accuracy: 0.7799\n",
            "Epoch 783/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8582 - val_loss: 9.5928 - val_accuracy: 0.7799\n",
            "Epoch 784/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5868 - val_accuracy: 0.7799\n",
            "Epoch 785/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8570 - val_loss: 9.5890 - val_accuracy: 0.7799\n",
            "Epoch 786/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8594 - val_loss: 9.5775 - val_accuracy: 0.7799\n",
            "Epoch 787/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.5864 - val_accuracy: 0.7799\n",
            "Epoch 788/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.5932 - val_accuracy: 0.7799\n",
            "Epoch 789/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8582 - val_loss: 9.5503 - val_accuracy: 0.7847\n",
            "Epoch 790/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8582 - val_loss: 9.5640 - val_accuracy: 0.7799\n",
            "Epoch 791/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3140 - accuracy: 0.8582 - val_loss: 9.5662 - val_accuracy: 0.7847\n",
            "Epoch 792/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8582 - val_loss: 9.5623 - val_accuracy: 0.7799\n",
            "Epoch 793/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8570 - val_loss: 9.5603 - val_accuracy: 0.7799\n",
            "Epoch 794/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3140 - accuracy: 0.8558 - val_loss: 9.5728 - val_accuracy: 0.7799\n",
            "Epoch 795/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.5690 - val_accuracy: 0.7799\n",
            "Epoch 796/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8594 - val_loss: 9.5909 - val_accuracy: 0.7799\n",
            "Epoch 797/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8582 - val_loss: 9.5885 - val_accuracy: 0.7799\n",
            "Epoch 798/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.6007 - val_accuracy: 0.7799\n",
            "Epoch 799/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8558 - val_loss: 9.5969 - val_accuracy: 0.7799\n",
            "Epoch 800/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.5940 - val_accuracy: 0.7799\n",
            "Epoch 801/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.6003 - val_accuracy: 0.7799\n",
            "Epoch 802/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.5992 - val_accuracy: 0.7799\n",
            "Epoch 803/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.6090 - val_accuracy: 0.7799\n",
            "Epoch 804/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6088 - val_accuracy: 0.7799\n",
            "Epoch 805/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8570 - val_loss: 9.6140 - val_accuracy: 0.7799\n",
            "Epoch 806/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.6063 - val_accuracy: 0.7799\n",
            "Epoch 807/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.6048 - val_accuracy: 0.7847\n",
            "Epoch 808/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8582 - val_loss: 9.6025 - val_accuracy: 0.7799\n",
            "Epoch 809/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8582 - val_loss: 9.6057 - val_accuracy: 0.7799\n",
            "Epoch 810/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.6015 - val_accuracy: 0.7799\n",
            "Epoch 811/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8582 - val_loss: 9.5995 - val_accuracy: 0.7799\n",
            "Epoch 812/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8570 - val_loss: 9.6185 - val_accuracy: 0.7799\n",
            "Epoch 813/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6155 - val_accuracy: 0.7799\n",
            "Epoch 814/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8570 - val_loss: 9.6182 - val_accuracy: 0.7799\n",
            "Epoch 815/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.6199 - val_accuracy: 0.7799\n",
            "Epoch 816/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.5101 - val_accuracy: 0.7751\n",
            "Epoch 817/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8570 - val_loss: 9.5133 - val_accuracy: 0.7751\n",
            "Epoch 818/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3141 - accuracy: 0.8594 - val_loss: 9.5803 - val_accuracy: 0.7751\n",
            "Epoch 819/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6007 - val_accuracy: 0.7751\n",
            "Epoch 820/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8594 - val_loss: 9.5951 - val_accuracy: 0.7751\n",
            "Epoch 821/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.8594 - val_loss: 9.6248 - val_accuracy: 0.7751\n",
            "Epoch 822/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8594 - val_loss: 9.6249 - val_accuracy: 0.7751\n",
            "Epoch 823/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8582 - val_loss: 9.6263 - val_accuracy: 0.7751\n",
            "Epoch 824/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8558 - val_loss: 9.6274 - val_accuracy: 0.7799\n",
            "Epoch 825/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6229 - val_accuracy: 0.7799\n",
            "Epoch 826/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3138 - accuracy: 0.8570 - val_loss: 9.6209 - val_accuracy: 0.7799\n",
            "Epoch 827/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6293 - val_accuracy: 0.7799\n",
            "Epoch 828/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6271 - val_accuracy: 0.7799\n",
            "Epoch 829/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.6258 - val_accuracy: 0.7799\n",
            "Epoch 830/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6303 - val_accuracy: 0.7799\n",
            "Epoch 831/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6271 - val_accuracy: 0.7799\n",
            "Epoch 832/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6327 - val_accuracy: 0.7799\n",
            "Epoch 833/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3139 - accuracy: 0.8582 - val_loss: 9.6589 - val_accuracy: 0.7799\n",
            "Epoch 834/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8570 - val_loss: 9.6529 - val_accuracy: 0.7799\n",
            "Epoch 835/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.6543 - val_accuracy: 0.7847\n",
            "Epoch 836/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6573 - val_accuracy: 0.7799\n",
            "Epoch 837/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6519 - val_accuracy: 0.7799\n",
            "Epoch 838/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6569 - val_accuracy: 0.7799\n",
            "Epoch 839/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3136 - accuracy: 0.8594 - val_loss: 9.6567 - val_accuracy: 0.7847\n",
            "Epoch 840/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.6534 - val_accuracy: 0.7799\n",
            "Epoch 841/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6559 - val_accuracy: 0.7799\n",
            "Epoch 842/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3137 - accuracy: 0.8570 - val_loss: 9.6548 - val_accuracy: 0.7847\n",
            "Epoch 843/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.6382 - val_accuracy: 0.7799\n",
            "Epoch 844/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3134 - accuracy: 0.8582 - val_loss: 9.6453 - val_accuracy: 0.7847\n",
            "Epoch 845/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3138 - accuracy: 0.8582 - val_loss: 9.6909 - val_accuracy: 0.7799\n",
            "Epoch 846/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3138 - accuracy: 0.8582 - val_loss: 9.6849 - val_accuracy: 0.7799\n",
            "Epoch 847/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3136 - accuracy: 0.8558 - val_loss: 9.6670 - val_accuracy: 0.7799\n",
            "Epoch 848/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6711 - val_accuracy: 0.7799\n",
            "Epoch 849/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.8558 - val_loss: 9.6756 - val_accuracy: 0.7799\n",
            "Epoch 850/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8594 - val_loss: 9.6487 - val_accuracy: 0.7799\n",
            "Epoch 851/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3145 - accuracy: 0.8570 - val_loss: 9.6531 - val_accuracy: 0.7799\n",
            "Epoch 852/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.6602 - val_accuracy: 0.7847\n",
            "Epoch 853/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.6600 - val_accuracy: 0.7847\n",
            "Epoch 854/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.6674 - val_accuracy: 0.7799\n",
            "Epoch 855/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6721 - val_accuracy: 0.7847\n",
            "Epoch 856/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3134 - accuracy: 0.8582 - val_loss: 9.6650 - val_accuracy: 0.7799\n",
            "Epoch 857/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3140 - accuracy: 0.8582 - val_loss: 9.7053 - val_accuracy: 0.7799\n",
            "Epoch 858/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6936 - val_accuracy: 0.7799\n",
            "Epoch 859/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.6963 - val_accuracy: 0.7799\n",
            "Epoch 860/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.6979 - val_accuracy: 0.7799\n",
            "Epoch 861/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3135 - accuracy: 0.8570 - val_loss: 9.6988 - val_accuracy: 0.7799\n",
            "Epoch 862/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.6961 - val_accuracy: 0.7847\n",
            "Epoch 863/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3135 - accuracy: 0.8570 - val_loss: 9.6924 - val_accuracy: 0.7799\n",
            "Epoch 864/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.6882 - val_accuracy: 0.7799\n",
            "Epoch 865/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3136 - accuracy: 0.8558 - val_loss: 9.7254 - val_accuracy: 0.7847\n",
            "Epoch 866/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3137 - accuracy: 0.8570 - val_loss: 9.7041 - val_accuracy: 0.7799\n",
            "Epoch 867/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3134 - accuracy: 0.8582 - val_loss: 9.7048 - val_accuracy: 0.7847\n",
            "Epoch 868/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8570 - val_loss: 9.6930 - val_accuracy: 0.7799\n",
            "Epoch 869/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8558 - val_loss: 9.7010 - val_accuracy: 0.7799\n",
            "Epoch 870/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.7054 - val_accuracy: 0.7847\n",
            "Epoch 871/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.6948 - val_accuracy: 0.7847\n",
            "Epoch 872/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8558 - val_loss: 9.7001 - val_accuracy: 0.7799\n",
            "Epoch 873/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.7037 - val_accuracy: 0.7799\n",
            "Epoch 874/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8582 - val_loss: 9.7057 - val_accuracy: 0.7847\n",
            "Epoch 875/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.7055 - val_accuracy: 0.7847\n",
            "Epoch 876/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.5484 - val_accuracy: 0.7799\n",
            "Epoch 877/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.5844 - val_accuracy: 0.7799\n",
            "Epoch 878/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.6532 - val_accuracy: 0.7847\n",
            "Epoch 879/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8594 - val_loss: 9.6991 - val_accuracy: 0.7751\n",
            "Epoch 880/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3137 - accuracy: 0.8594 - val_loss: 9.6930 - val_accuracy: 0.7799\n",
            "Epoch 881/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8570 - val_loss: 9.6945 - val_accuracy: 0.7799\n",
            "Epoch 882/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.6933 - val_accuracy: 0.7799\n",
            "Epoch 883/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.6923 - val_accuracy: 0.7799\n",
            "Epoch 884/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8570 - val_loss: 9.6966 - val_accuracy: 0.7799\n",
            "Epoch 885/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.6958 - val_accuracy: 0.7799\n",
            "Epoch 886/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8570 - val_loss: 9.6874 - val_accuracy: 0.7799\n",
            "Epoch 887/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8582 - val_loss: 9.7019 - val_accuracy: 0.7847\n",
            "Epoch 888/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8582 - val_loss: 9.6901 - val_accuracy: 0.7799\n",
            "Epoch 889/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3139 - accuracy: 0.8594 - val_loss: 9.7001 - val_accuracy: 0.7799\n",
            "Epoch 890/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.7016 - val_accuracy: 0.7799\n",
            "Epoch 891/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8594 - val_loss: 9.7095 - val_accuracy: 0.7799\n",
            "Epoch 892/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8582 - val_loss: 9.7161 - val_accuracy: 0.7799\n",
            "Epoch 893/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8582 - val_loss: 9.7147 - val_accuracy: 0.7799\n",
            "Epoch 894/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.7176 - val_accuracy: 0.7847\n",
            "Epoch 895/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3132 - accuracy: 0.8582 - val_loss: 9.7562 - val_accuracy: 0.7799\n",
            "Epoch 896/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8594 - val_loss: 9.7407 - val_accuracy: 0.7799\n",
            "Epoch 897/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8582 - val_loss: 9.7398 - val_accuracy: 0.7799\n",
            "Epoch 898/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.7263 - val_accuracy: 0.7799\n",
            "Epoch 899/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8582 - val_loss: 9.7324 - val_accuracy: 0.7799\n",
            "Epoch 900/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.6989 - val_accuracy: 0.7799\n",
            "Epoch 901/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3141 - accuracy: 0.8582 - val_loss: 9.7137 - val_accuracy: 0.7799\n",
            "Epoch 902/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.7166 - val_accuracy: 0.7847\n",
            "Epoch 903/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8606 - val_loss: 9.7228 - val_accuracy: 0.7799\n",
            "Epoch 904/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8570 - val_loss: 9.7236 - val_accuracy: 0.7847\n",
            "Epoch 905/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8570 - val_loss: 9.7246 - val_accuracy: 0.7799\n",
            "Epoch 906/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8570 - val_loss: 9.7227 - val_accuracy: 0.7847\n",
            "Epoch 907/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8594 - val_loss: 9.7137 - val_accuracy: 0.7799\n",
            "Epoch 908/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8582 - val_loss: 9.7572 - val_accuracy: 0.7847\n",
            "Epoch 909/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8582 - val_loss: 9.7439 - val_accuracy: 0.7799\n",
            "Epoch 910/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8594 - val_loss: 9.7386 - val_accuracy: 0.7799\n",
            "Epoch 911/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8594 - val_loss: 9.7417 - val_accuracy: 0.7799\n",
            "Epoch 912/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8570 - val_loss: 9.7487 - val_accuracy: 0.7847\n",
            "Epoch 913/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8582 - val_loss: 9.7586 - val_accuracy: 0.7847\n",
            "Epoch 914/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.7589 - val_accuracy: 0.7799\n",
            "Epoch 915/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8570 - val_loss: 9.7573 - val_accuracy: 0.7799\n",
            "Epoch 916/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8582 - val_loss: 9.7543 - val_accuracy: 0.7799\n",
            "Epoch 917/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.7595 - val_accuracy: 0.7799\n",
            "Epoch 918/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3131 - accuracy: 0.8582 - val_loss: 9.7545 - val_accuracy: 0.7847\n",
            "Epoch 919/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8582 - val_loss: 9.7567 - val_accuracy: 0.7847\n",
            "Epoch 920/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8594 - val_loss: 9.7507 - val_accuracy: 0.7799\n",
            "Epoch 921/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.7631 - val_accuracy: 0.7799\n",
            "Epoch 922/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8582 - val_loss: 9.7574 - val_accuracy: 0.7751\n",
            "Epoch 923/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.7880 - val_accuracy: 0.7799\n",
            "Epoch 924/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8594 - val_loss: 9.7825 - val_accuracy: 0.7799\n",
            "Epoch 925/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3142 - accuracy: 0.8594 - val_loss: 9.7280 - val_accuracy: 0.7751\n",
            "Epoch 926/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8570 - val_loss: 9.7443 - val_accuracy: 0.7799\n",
            "Epoch 927/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3137 - accuracy: 0.8582 - val_loss: 9.7536 - val_accuracy: 0.7799\n",
            "Epoch 928/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8582 - val_loss: 9.7604 - val_accuracy: 0.7799\n",
            "Epoch 929/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8582 - val_loss: 9.7586 - val_accuracy: 0.7847\n",
            "Epoch 930/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3131 - accuracy: 0.8594 - val_loss: 9.7587 - val_accuracy: 0.7799\n",
            "Epoch 931/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3130 - accuracy: 0.8594 - val_loss: 9.7707 - val_accuracy: 0.7799\n",
            "Epoch 932/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.8570 - val_loss: 9.7910 - val_accuracy: 0.7847\n",
            "Epoch 933/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8594 - val_loss: 9.7836 - val_accuracy: 0.7799\n",
            "Epoch 934/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8606 - val_loss: 9.7761 - val_accuracy: 0.7751\n",
            "Epoch 935/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8582 - val_loss: 9.7828 - val_accuracy: 0.7799\n",
            "Epoch 936/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8582 - val_loss: 9.7866 - val_accuracy: 0.7799\n",
            "Epoch 937/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3130 - accuracy: 0.8570 - val_loss: 9.7784 - val_accuracy: 0.7799\n",
            "Epoch 938/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8582 - val_loss: 9.7744 - val_accuracy: 0.7799\n",
            "Epoch 939/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8582 - val_loss: 9.7814 - val_accuracy: 0.7799\n",
            "Epoch 940/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3131 - accuracy: 0.8558 - val_loss: 9.7941 - val_accuracy: 0.7799\n",
            "Epoch 941/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3130 - accuracy: 0.8582 - val_loss: 9.8025 - val_accuracy: 0.7799\n",
            "Epoch 942/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3131 - accuracy: 0.8570 - val_loss: 9.6797 - val_accuracy: 0.7799\n",
            "Epoch 943/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3138 - accuracy: 0.8582 - val_loss: 9.6860 - val_accuracy: 0.7799\n",
            "Epoch 944/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3136 - accuracy: 0.8594 - val_loss: 9.6999 - val_accuracy: 0.7799\n",
            "Epoch 945/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3133 - accuracy: 0.8594 - val_loss: 9.7640 - val_accuracy: 0.7799\n",
            "Epoch 946/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3130 - accuracy: 0.8582 - val_loss: 9.7679 - val_accuracy: 0.7799\n",
            "Epoch 947/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3130 - accuracy: 0.8582 - val_loss: 9.7699 - val_accuracy: 0.7799\n",
            "Epoch 948/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3131 - accuracy: 0.8570 - val_loss: 9.7741 - val_accuracy: 0.7799\n",
            "Epoch 949/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3130 - accuracy: 0.8606 - val_loss: 9.7832 - val_accuracy: 0.7799\n",
            "Epoch 950/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3131 - accuracy: 0.8558 - val_loss: 9.7755 - val_accuracy: 0.7799\n",
            "Epoch 951/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3131 - accuracy: 0.8570 - val_loss: 9.7822 - val_accuracy: 0.7799\n",
            "Epoch 952/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3132 - accuracy: 0.8570 - val_loss: 9.8299 - val_accuracy: 0.7751\n",
            "Epoch 953/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.8191 - val_accuracy: 0.7799\n",
            "Epoch 954/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3131 - accuracy: 0.8594 - val_loss: 9.7971 - val_accuracy: 0.7799\n",
            "Epoch 955/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3132 - accuracy: 0.8594 - val_loss: 9.8030 - val_accuracy: 0.7799\n",
            "Epoch 956/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3131 - accuracy: 0.8594 - val_loss: 9.8029 - val_accuracy: 0.7799\n",
            "Epoch 957/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3129 - accuracy: 0.8594 - val_loss: 9.8059 - val_accuracy: 0.7799\n",
            "Epoch 958/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3128 - accuracy: 0.8582 - val_loss: 9.8233 - val_accuracy: 0.7799\n",
            "Epoch 959/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3128 - accuracy: 0.8594 - val_loss: 9.8113 - val_accuracy: 0.7799\n",
            "Epoch 960/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3128 - accuracy: 0.8594 - val_loss: 9.8110 - val_accuracy: 0.7799\n",
            "Epoch 961/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3126 - accuracy: 0.8582 - val_loss: 9.8141 - val_accuracy: 0.7799\n",
            "Epoch 962/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3129 - accuracy: 0.8594 - val_loss: 9.7989 - val_accuracy: 0.7799\n",
            "Epoch 963/1000\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.3126 - accuracy: 0.8594 - val_loss: 9.8127 - val_accuracy: 0.7799\n",
            "Epoch 964/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3129 - accuracy: 0.8606 - val_loss: 9.8201 - val_accuracy: 0.7799\n",
            "Epoch 965/1000\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3125 - accuracy: 0.8582 - val_loss: 9.6603 - val_accuracy: 0.7799\n",
            "Epoch 966/1000\n",
            "26/26 [==============================] - 0s 7ms/step - loss: 0.3137 - accuracy: 0.8594 - val_loss: 9.6997 - val_accuracy: 0.7799\n",
            "Epoch 967/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8582 - val_loss: 9.7189 - val_accuracy: 0.7799\n",
            "Epoch 968/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8594 - val_loss: 9.7265 - val_accuracy: 0.7799\n",
            "Epoch 969/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8594 - val_loss: 9.8083 - val_accuracy: 0.7799\n",
            "Epoch 970/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3130 - accuracy: 0.8582 - val_loss: 9.7987 - val_accuracy: 0.7799\n",
            "Epoch 971/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.8594 - val_loss: 9.7941 - val_accuracy: 0.7799\n",
            "Epoch 972/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3129 - accuracy: 0.8594 - val_loss: 9.8010 - val_accuracy: 0.7799\n",
            "Epoch 973/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.8606 - val_loss: 9.8122 - val_accuracy: 0.7799\n",
            "Epoch 974/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3128 - accuracy: 0.8594 - val_loss: 9.7966 - val_accuracy: 0.7799\n",
            "Epoch 975/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3128 - accuracy: 0.8594 - val_loss: 9.8149 - val_accuracy: 0.7799\n",
            "Epoch 976/1000\n",
            "26/26 [==============================] - 0s 4ms/step - loss: 0.3125 - accuracy: 0.8582 - val_loss: 9.8230 - val_accuracy: 0.7799\n",
            "Epoch 977/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3129 - accuracy: 0.8594 - val_loss: 9.8137 - val_accuracy: 0.7799\n",
            "Epoch 978/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3134 - accuracy: 0.8594 - val_loss: 9.7976 - val_accuracy: 0.7799\n",
            "Epoch 979/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3131 - accuracy: 0.8570 - val_loss: 9.8078 - val_accuracy: 0.7799\n",
            "Epoch 980/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3135 - accuracy: 0.8594 - val_loss: 9.8390 - val_accuracy: 0.7799\n",
            "Epoch 981/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3129 - accuracy: 0.8594 - val_loss: 9.8282 - val_accuracy: 0.7799\n",
            "Epoch 982/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8582 - val_loss: 9.8369 - val_accuracy: 0.7799\n",
            "Epoch 983/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.8594 - val_loss: 9.8408 - val_accuracy: 0.7799\n",
            "Epoch 984/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8594 - val_loss: 9.8432 - val_accuracy: 0.7799\n",
            "Epoch 985/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.8594 - val_loss: 9.8565 - val_accuracy: 0.7799\n",
            "Epoch 986/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8594 - val_loss: 9.8492 - val_accuracy: 0.7799\n",
            "Epoch 987/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8594 - val_loss: 9.8553 - val_accuracy: 0.7799\n",
            "Epoch 988/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3126 - accuracy: 0.8582 - val_loss: 9.8558 - val_accuracy: 0.7799\n",
            "Epoch 989/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3123 - accuracy: 0.8582 - val_loss: 9.8644 - val_accuracy: 0.7799\n",
            "Epoch 990/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.8594 - val_loss: 9.9000 - val_accuracy: 0.7799\n",
            "Epoch 991/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3128 - accuracy: 0.8594 - val_loss: 9.8872 - val_accuracy: 0.7799\n",
            "Epoch 992/1000\n",
            "26/26 [==============================] - 0s 6ms/step - loss: 0.3126 - accuracy: 0.8582 - val_loss: 9.8813 - val_accuracy: 0.7799\n",
            "Epoch 993/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3120 - accuracy: 0.8582 - val_loss: 9.7399 - val_accuracy: 0.7799\n",
            "Epoch 994/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8606 - val_loss: 9.7627 - val_accuracy: 0.7799\n",
            "Epoch 995/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.8582 - val_loss: 9.8346 - val_accuracy: 0.7799\n",
            "Epoch 996/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8594 - val_loss: 9.8339 - val_accuracy: 0.7799\n",
            "Epoch 997/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3127 - accuracy: 0.8582 - val_loss: 9.8334 - val_accuracy: 0.7799\n",
            "Epoch 998/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.8582 - val_loss: 9.8271 - val_accuracy: 0.7799\n",
            "Epoch 999/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8582 - val_loss: 9.8292 - val_accuracy: 0.7799\n",
            "Epoch 1000/1000\n",
            "26/26 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.8582 - val_loss: 9.8532 - val_accuracy: 0.7799\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d6064b54d30>"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (model.predict(X_test)> 0.5).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAMzQ-Qph0jj",
        "outputId": "ddbd3aa2-7386-4fa0-82ce-c0f7f557df4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "class_report = classification_report(y_test,y_pred)\n",
        "print(f'Classification Report:\\n{class_report}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ_aGL36h56l",
        "outputId": "f276a1e3-19fa-4191-fa2e-4e468fe9a0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 75.62%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.73      0.74       270\n",
            "           1       0.76      0.78      0.77       292\n",
            "\n",
            "    accuracy                           0.76       562\n",
            "   macro avg       0.76      0.76      0.76       562\n",
            "weighted avg       0.76      0.76      0.76       562\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "JHRRmNeTh-CV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e72b6b-4e2b-4256-c716-db715a7a7bad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[198  72]\n",
            " [ 65 227]]\n"
          ]
        }
      ]
    }
  ]
}